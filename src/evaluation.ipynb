{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üç´Tune your RAG data pipeline and evaluate its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è This notebook can be run on your local machine or on a virtual machine and requires [Docker Compose](https://docs.docker.com/desktop/).\n",
    "> Please note that it is unfortunately **not compatible with Google Colab** as the latter does not support Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° This notebook allows you to iteratively configure and run a RAG pipeline. Check out our [advanced notebook](./parameter_search.ipynb) if you want to perform parameter search and launch multiple runs at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how to iteratively evaluate and tune a Retrieval-Augmented Generation (RAG) system using [Fondant](https://fondant.ai).\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Set up a [Weaviate](https://weaviate.io/platform) vector store\n",
    "2. Define a parameter set to test\n",
    "3. Run a Fondant pipeline with those parameters to index our documents into the vector store\n",
    "4. Run a Fondant pipeline with those parameters to evaluate the performance\n",
    "5. Inspect the evaluation results and data between each processing step\n",
    "6. Repeat step 2 - 5 until we're happy with the results\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../art/iteration.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [**Fondant**](https://github.com/ml6team/fondant), a hub and framework for easy and shareable data processing, as it has the following advantages for RAG evaluation:\n",
    "\n",
    "- **Speed**\n",
    "    - Leverage reusable RAG components from the [Fondant Hub](https://fondant.ai/en/latest/components/hub/) to quickly build RAG pipelines\n",
    "    - [Pipeline caching](https://fondant.ai/en/latest/caching/) to speed up iteration on subsequent runs\n",
    "    - Parallel processing out of the box to speed up processing of large datasets especially\n",
    "    - Local development with the Docker Compose runner (used in this notebook)\n",
    "- **Ease-of-use**\n",
    "    - Easily adaptable: change parameters and swap [components](https://fondant.ai/en/latest/components/hub/) by changing only a few lines of code\n",
    "    - Easily extendable: create your own [custom components](https://fondant.ai/en/latest/components/custom_component/) (eg. with different chunking strategies) and plug them into your pipeline\n",
    "    - Reusable & shareable: reuse your processing components in different pipelines and share them with the [community](https://discord.gg/HnTdWhydGp)\n",
    "- **Production-readiness**\n",
    "    - Pipeline with dockerized steps ready to deploy to (managed) platforms such as _Vertex, SageMaker and Kubeflow_\n",
    "    - Full data lineage and a [data explorer](https://fondant.ai/en/latest/data_explorer/) to check the evolution of data after each step\n",
    "    - Ready to deploy to (managed) platforms such as _Vertex, SageMaker and Kubeflow_\n",
    " \n",
    "Please share your experiences or let us know how we can improve through our [**Discord**](https://discord.gg/HnTdWhydGp) or on [**GitHub**](https://github.com/ml6team/fondant). And of course feel free to give us a [**star ‚≠ê**](https://github.com/ml6team/fondant) if you like what we are doing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è This section checks the prerequisites of your environment. Read any errors or warnings carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure a Python between version 3.8 and 3.10 is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info < (3, 8, 0) or sys.version_info >= (3, 11, 0):\n",
    "    raise Exception(f\"A Python version between 3.8 and 3.10 is required. You are running {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if docker compose is installed and the docker daemon is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Fondant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r ../requirements.txt --disable-pip-version-check && echo \"Success\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spin up the Weaviate vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è For Apple M1/M2 chip users:\n",
    "> \n",
    "> - In Docker Desktop Dashboard `Settings -> Features in development`, make sure to **un**check `Use containerd` for pulling and storing images. More info [here](https://docs.docker.com/desktop/settings/mac/#beta-features)\n",
    "> - Make sure that Docker uses linux/amd64 platform and not arm64 (cell below should take care of that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DOCKER_DEFAULT_PLATFORM\"]=\"linux/amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Weaviate with Docker compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f weaviate/docker-compose.yaml up --detach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have Weaviate client v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"weaviate-client==3.*\" --disable-pip-version-check && echo \"Weaviate client installed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the vectorDB is running and accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import weaviate\n",
    "\n",
    "try:\n",
    "    local_weaviate_client = weaviate.Client(\"http://localhost:8080\")\n",
    "    logging.info(\"Connected to Weaviate instance\")\n",
    "except weaviate.WeaviateStartUpError:\n",
    "    logging.error(\"Cannot connect to weaviate instance, is it running?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline, which can be found in [`pipeline_index.py`](./pipeline_index.py) processes text data and loads it into the vector database. It consists of the following steps:\n",
    "<div align=center>\n",
    "<img src=\"../art/indexing_ltr.png\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "- [**HF Data Loading**](https://github.com/ml6team/fondant/tree/main/components/load_from_parquet): loads data from the Hugging Face Hub.\n",
    "- [**Text Chunking**](https://github.com/ml6team/fondant/tree/main/components/chunk_text): divides the text into sections of a certain size and with a certain overlap\n",
    "- [**Text Embedding**](https://github.com/ml6team/fondant/tree/main/components/embed_text): embeds each chunk as a vector.  \n",
    "  üí° Can use different models / APIs. When using a HuggingFace model (the default), use a machine with GPU for large datasets.\n",
    "- [**Write to Weaviate**](https://github.com/ml6team/fondant/tree/main/components/index_weaviate): writes data and embeddings to the vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° This notebook defaults to the first 1000 rows of the [wikitext](https://huggingface.co/datasets/wikitext) dataset for demonstration purposes, but you can load your own dataset using one the other load components available on the [**Fondant Hub**](https://fondant.ai/en/latest/components/hub/#component-hub) or by creating your own [**custom load component**](https://fondant.ai/en/latest/guides/implement_custom_components/). Keep in mind that changing the dataset implies that you also need to change the evaluation dataset used in the evaluation pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the indexing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the arguments below if you want to run the pipeline with different parameters. For more information on the possible values, check out the above links to the component documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline_index\n",
    "import utils\n",
    "\n",
    "# Path where data and artifacts will be stored\n",
    "BASE_PATH = \"./data-dir\"\n",
    "utils.create_directory_if_not_exists(BASE_PATH)\n",
    "\n",
    "# Parameters shared between indexing and evaluation pipeline\n",
    "shared_args = {\n",
    "    \"base_path\": BASE_PATH,\n",
    "    \"embed_model_provider\": \"huggingface\",\n",
    "    \"embed_model\": \"all-MiniLM-L6-v2\",\n",
    "    \"embed_api_key\": {},\n",
    "    \"weaviate_url\": f\"http://{utils.get_host_ip()}:8080\",\n",
    "    \"weaviate_class\": \"Pipeline1\", # Capitalized, avoid special characters (_, =, -, etc.)\n",
    "}\n",
    "\n",
    "# Parameters for the indexing pipeline\n",
    "indexing_args = {\n",
    "    \"n_rows_to_load\": 1000,\n",
    "    \"chunk_size\": 1024,\n",
    "    \"chunk_overlap\": 8,\n",
    "}\n",
    "\n",
    "indexing_pipeline = pipeline_index.create_pipeline(**shared_args, **indexing_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the indexing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° The first time you run a pipeline, you need to download a docker image for each component which may take a minute. The second time, processing will start instantly. Moreover, steps that have been processed before are cached and will be skipped in subsequent runs which will speed things up even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.pipeline.runner import DockerRunner\n",
    "\n",
    "runner = DockerRunner()\n",
    "runner.run(indexing_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline, which can be found in `pipeline_eval.py` evaluates retrieval performance using the questions provided in your test dataset. It consists of multiple steps/components:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../art/evaluation_ltr.png\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "- [**CSV Data Loading**](https://github.com/ml6team/fondant/tree/main/components/load_from_csv): loads the evaluation dataset (questions) from a csv file.\n",
    "- [**Text Embedding**](https://github.com/ml6team/fondant/tree/main/components/embed_text): embeds each chunk as a vector.  \n",
    "  üí° Can use different models / APIs. When using a HuggingFace model (the default), use a machine with GPU for large datasets.\n",
    "- [**Vector store Retrieval**](https://github.com/ml6team/fondant/tree/main/components/retrieve_from_weaviate): retrieves the most relevant chunks for each question from the vector store.\n",
    "- [**Ragas evaluation**](https://github.com/ml6team/fondant/tree/0.8.0/components/evaluate_ragas): evaluates the retrieved chunks for each question with [RAGAS](https://docs.ragas.io/en/latest/index.html).\n",
    "- [**Aggregate metrics**](https://github.com/ml6team/fondant-usecase-RAG/tree/main/src/components/aggregate_eval_results): Aggregate the results on a pipeline level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è If you want to use OpenAI you will need an [OpenAI API key](https://platform.openai.com/docs/quickstart) (see TODO below)\n",
    "\n",
    "Change the arguments below if you want to run the pipeline with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pipeline_eval\n",
    "\n",
    "evaluation_args = {\n",
    "    \"retrieval_top_k\": 2,\n",
    "    \"evaluation_module\": \"langchain.llms\",\n",
    "    \"evaluation_llm\": \"OpenAI\",\n",
    "    \"evaluation_llm_kwargs\": {\"openai_api_key\": os.environ[\"OPENAI_KEY\"]},  # TODO: Update with your key or use a different model\n",
    "    \"evaluation_metrics\": [\"context_precision\", \"context_relevancy\"]\n",
    "}\n",
    "\n",
    "evaluation_pipeline = pipeline_eval.create_pipeline(**shared_args, **evaluation_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the evaluation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = DockerRunner()\n",
    "extra_volumes = [str(os.path.join(os.path.abspath('.'), \"evaluation_datasets\")) + \":/data\"]\n",
    "runner.run(evaluation_pipeline, extra_volumes=extra_volumes)\n",
    "\n",
    "utils.store_results(evaluation_pipeline.name, **shared_args, **indexing_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.read_results(evaluation_pipeline.name, base_path=BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check your data and results at each step in the pipelines using the Fondant data explorer. The first time you run the data explorer, you need to download the docker image which may take a minute. Afterwards you can access the data explorer at:\n",
    "\n",
    "**http://localhost:8501/**\n",
    "\n",
    "Enjoy the exploration! üç´ \n",
    "\n",
    "Press the ‚óºÔ∏è in the notebook toolbar to stop the explorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.explore import run_explorer_app\n",
    "\n",
    "run_explorer_app(base_path=BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up your environment\n",
    "\n",
    "After your pipeline ran successfully, you should clean up your environment and stop the weaviate database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f weaviate/docker-compose.yaml down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please share your experience or let us know how we can improve through our \n",
    "* [**Discord**](https://discord.gg/HnTdWhydGp) \n",
    "* [**GitHub**](https://github.com/ml6team/fondant)\n",
    "\n",
    "And of course feel free to give us a [**star** ‚≠ê](https://github.com/ml6team/fondant) if you like what we are doing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
