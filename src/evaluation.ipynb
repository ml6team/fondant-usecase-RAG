{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üç´Tune your RAG data pipeline and evaluate its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è This notebook can be run on your local machine or on a virtual machine and requires [Docker Compose](https://docs.docker.com/desktop/).\n",
    "> Please note that it is not compatible with Google Colab as the latter does not support Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how to iteratively evaluate and tune a Retrieval-Augmented Generation (RAG) system using [Fondant](https://fondant.ai).\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Set up a [Weaviate](https://weaviate.io/platform) vector store\n",
    "2. Define a parameter set to test\n",
    "3. Run a Fondant pipeline with those parameters to index our documents into the vector store\n",
    "4. Run a Fondant pipeline with those parameters to evaluate the performance\n",
    "5. Inspect the evaluation results and data between each processing step\n",
    "6. Repeat step 2 - 5 until we're happy with the results\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../art/iteration.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è This section checks the prerequisites of your environment. Read any errors or warnings carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure a **Python between version 3.8 and 3.10** is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info < (3, 8, 0) or sys.version_info >= (3, 11, 0):\n",
    "    raise Exception(f\"A Python version between 3.8 and 3.10 is required. You are running {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if **docker compose** is installed and the **docker daemon** is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Fondant framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r ../requirements.txt --disable-pip-version-check && echo \"Success\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if GPU is available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    subprocess.check_output('nvidia-smi')\n",
    "    logging.info(\"Found GPU, using it!\")\n",
    "    number_of_accelerators = 1\n",
    "    accelerator_name = \"GPU\"\n",
    "except Exception:\n",
    "    logging.warning(\"We recommend to run this pipeline on a GPU, but none could be found, using CPU instead\")\n",
    "    number_of_accelerators = None\n",
    "    accelerator_name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spin up the Weaviate vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è For **Apple M1/M2** chip users:\n",
    "> \n",
    "> - In Docker Desktop Dashboard `Settings -> Features in development`, make sure to **un**check `Use containerd` for pulling and storing images. More info [here](https://docs.docker.com/desktop/settings/mac/#beta-features)\n",
    "> - Make sure that Docker uses linux/amd64 platform and not arm64 (cell below should take care of that)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run **Weaviate** with Docker compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f weaviate_service/docker-compose.yaml up --detach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have **Weaviate client v3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the vectorDB is running and accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import weaviate\n",
    "\n",
    "try:\n",
    "    local_weaviate_client = weaviate.Client(\"http://localhost:8081\")\n",
    "    logging.info(\"Connected to Weaviate instance\")\n",
    "except weaviate.WeaviateStartUpError:\n",
    "    logging.error(\"Cannot connect to weaviate instance, is it running?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pipeline_index.py` processes text data and loads it into the vector database\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../art/indexing_ltr.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "- [**Load data**](https://github.com/ml6team/fondant/tree/main/components/load_from_parquet): loads data from the Hugging Face Hub\n",
    "- [**Chunk data**](https://github.com/ml6team/fondant/tree/main/components/chunk_text): divides the text into sections of a certain size and with a certain overlap\n",
    "- [**Embed chunks**](https://github.com/ml6team/fondant/tree/main/components/embed_text): embeds each chunk as a vector, e.g. using [Cohere](https://cohere.com/embeddings)\n",
    "- [**Index vector store**](https://github.com/ml6team/fondant/tree/main/components/index_weaviate): writes data and embeddings to the vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° This notebook defaults to the first 1000 rows of the [wikitext](https://huggingface.co/datasets/wikitext) dataset for demonstration purposes, but you can load your own dataset using one the other load components available on the [**Fondant Hub**](https://fondant.ai/en/latest/components/hub/#component-hub) or by creating your own [**custom load component**](https://fondant.ai/en/latest/guides/implement_custom_components/). Keep in mind that changing the dataset implies that you also need to change the evaluation dataset used in the evaluation pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the indexing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are reusing the index pipeline from the [indexing notebook](./indexing.ipynb). Therefore, we have extracted the code into a separate file and created a function that parameterizes the entire pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline_index\n",
    "import utils\n",
    "\n",
    "# Path where data and artifacts will be stored\n",
    "BASE_PATH = \"./data\"\n",
    "utils.create_directory_if_not_exists(BASE_PATH)\n",
    "\n",
    "# Parameters shared between indexing and evaluation pipeline\n",
    "shared_args = {\n",
    "    \"base_path\": BASE_PATH,\n",
    "    \"embed_model_provider\": \"huggingface\",\n",
    "    \"embed_model\": \"all-MiniLM-L6-v2\",\n",
    "    \"weaviate_url\": f\"http://{utils.get_host_ip()}:8081\",\n",
    "    \"weaviate_class\": \"Pipeline1\", # Capitalized, avoid special characters (_, =, -, etc.)\n",
    "}\n",
    "\n",
    "# Parameters for the indexing pipeline\n",
    "indexing_args = {\n",
    "    \"n_rows_to_load\": 1000,\n",
    "    \"chunk_args\": {\"chunk_size\": 512, \"chunk_overlap\": 32}\n",
    "}\n",
    "\n",
    "# Parameters for the GPU resources\n",
    "resources_args = {\n",
    "    \"number_of_accelerators\": number_of_accelerators,\n",
    "    \"accelerator_name\": accelerator_name,\n",
    "}\n",
    "\n",
    "indexing_pipeline = pipeline_index.create_pipeline(**shared_args, **indexing_args, **resources_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the indexing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.pipeline.runner import DockerRunner\n",
    "\n",
    "runner = DockerRunner()\n",
    "runner.run(indexing_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pipeline_eval.py` evaluates retrieval performance using the questions provided in your test dataset\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"../art/evaluation_ltr.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "- [**Load eval data**](https://github.com/ml6team/fondant/tree/main/components/load_from_csv): loads the evaluation dataset (questions) from a csv file\n",
    "- [**Embed questons**](https://github.com/ml6team/fondant/tree/main/components/embed_text): embeds each question as a vector, e.g. using [Cohere](https://cohere.com/embeddings)\n",
    "- [**Query vector store**](https://github.com/ml6team/fondant/tree/main/components/retrieve_from_weaviate): retrieves the most relevant chunks for each question from the vector store\n",
    "- [**Evaluate**](https://github.com/ml6team/fondant/tree/0.8.0/components/evaluate_ragas): evaluates the retrieved chunks for each question, e.g. using [RAGAS](https://docs.ragas.io/en/latest/index.html)\n",
    "- [**Aggregate**](https://github.com/ml6team/fondant-usecase-RAG/tree/main/src/components/aggregate_eval_results): calculates aggregated results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è If you want to use an **OpenAI** model for evaluation you will need an [API key](https://platform.openai.com/docs/quickstart) (see TODO below)\n",
    "\n",
    "Change the arguments below if you want to run the pipeline with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_args = {\n",
    "    \"retrieval_top_k\": 2,\n",
    "    \"llm_module_name\": \"langchain.chat_models\",\n",
    "    \"llm_class_name\": \"ChatOpenAI\",\n",
    "    \"llm_kwargs\": {\n",
    "      \"openai_api_key\": \"\" ,   # TODO: Update with your key or use a different model\n",
    "      \"model_name\" : \"gpt-3.5-turbo\"\n",
    "    },\n",
    "    \"evaluation_metrics\": [\"context_precision\", \"context_relevancy\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by initializing our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from fondant.pipeline import Pipeline\n",
    "evaluation_pipeline = Pipeline(\n",
    "        name=\"evaluation-pipeline\",\n",
    "        description=\"Pipeline to evaluate a RAG solution\",\n",
    "        base_path=shared_args[\"base_path\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have created a set of evaluation questions which we will use to evaluate the retrieval performance of the RAG system. Therefore, we need to load the CSV file containing the questions. We are going to use a reusable component for this task, `load_from_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_set_filename = \"wikitext_1000_q.csv\"\n",
    "\n",
    "load_from_csv = evaluation_pipeline.read(\n",
    "    \"load_from_csv\",\n",
    "    arguments={\n",
    "        \"dataset_uri\": \"/evaldata/\" + evaluation_set_filename,\n",
    "        # mounted dir from within docker as extra_volumes\n",
    "        \"column_separator\": \";\",\n",
    "    },\n",
    "    produces={\n",
    "        \"question\": pa.string(),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterward, we are going to embed our questions and retrieve answers from the database. Here we will once again use the reusable `embed_text` component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.pipeline import Resources\n",
    "embed_text_op = load_from_csv.apply(\n",
    "    \"embed_text\",\n",
    "    arguments={\n",
    "        \"model_provider\": shared_args[\"embed_model_provider\"],\n",
    "        \"model\": shared_args[\"embed_model\"]\n",
    "    },\n",
    "    consumes={\n",
    "        \"text\": \"question\",\n",
    "    },\n",
    "    resources=Resources(\n",
    "        accelerator_number=number_of_accelerators,\n",
    "        accelerator_name=accelerator_name,\n",
    "    ),\n",
    "    cluster_type=\"local\" if number_of_accelerators is not None else \"default\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we are going to retrieve chunks from the vector database and evaluate the retrieved chunks using RAGAS. Finally, we are going to aggregate the metrics to allow an overall performance estimation.\n",
    "\n",
    "Take a look at the `components` folder to learn more about the custom component implementation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from components.retrieve_from_weaviate import RetrieveFromWeaviateComponent\n",
    "from components.evaluate_ragas import RagasEvaluator\n",
    "from components.aggregrate_eval_results import AggregateResults\n",
    "\n",
    "retrieve_chunks = embed_text_op.apply(\n",
    "    RetrieveFromWeaviateComponent,\n",
    "    arguments={\n",
    "        \"weaviate_url\": shared_args[\"weaviate_url\"],\n",
    "        \"class_name\": shared_args[\"weaviate_class\"],\n",
    "        \"top_k\": 5\n",
    "    },\n",
    ")\n",
    "\n",
    "retriever_eval = retrieve_chunks.apply(\n",
    "    RagasEvaluator,\n",
    "    arguments={\n",
    "        \"llm_module_name\": evaluation_args[\"llm_module_name\"],\n",
    "        \"llm_class_name\": evaluation_args[\"llm_class_name\"],\n",
    "        \"llm_kwargs\": evaluation_args[\"llm_kwargs\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "retriever_eval.apply(\n",
    "    AggregateResults, \n",
    "    consumes={\n",
    "        \"context_precision\": \"context_precision\",\n",
    "        \"context_relevancy\": \"context_relevancy\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the evaluation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if utils.check_weaviate_class_exists(\n",
    "    local_weaviate_client,\n",
    "    shared_args[\"weaviate_class\"]\n",
    "): \n",
    "    runner = DockerRunner()\n",
    "    extra_volumes = [str(os.path.join(os.path.abspath('.'), \"evaluation_datasets\")) + \":/evaldata\"]\n",
    "    runner.run(evaluation_pipeline, extra_volumes=extra_volumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.get_metrics_latest_run(base_path=BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check your data and results at each step in the pipelines using the **Fondant data explorer**. The first time you run the data explorer, you need to download the docker image which may take a minute. Then you can access the data explorer at: **http://localhost:8501/**\n",
    "\n",
    "Enjoy the exploration! üç´ \n",
    "\n",
    "Press the ‚óºÔ∏è in the notebook toolbar to **stop the explorer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.explore import run_explorer_app\n",
    "\n",
    "run_explorer_app(base_path=BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop the Explore, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.explore import stop_explorer_app\n",
    "\n",
    "stop_explorer_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After your pipeline run successfully, you can **clean up** your environment and stop the weaviate database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f weaviate/docker-compose.yaml down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please share your experience or **let us know how we can improve** through our \n",
    "* [**Discord**](https://discord.gg/HnTdWhydGp) \n",
    "* [**GitHub**](https://github.com/ml6team/fondant)\n",
    "\n",
    "And of course feel free to give us a [**star** ‚≠ê](https://github.com/ml6team/fondant) if you like what we are doing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
