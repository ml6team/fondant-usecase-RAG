{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üç´Tune your RAG data pipeline and evaluate its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è This notebook can be run on your local machine or on a virtual machine and requires [Docker Compose](https://docs.docker.com/desktop/).\n",
    "> Please note that it is not compatible with Google Colab as the latter does not support Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° This notebook allows you to iteratively configure and run a RAG pipeline. Check out our [**advanced notebook**](./parameter_search.ipynb) if you want to perform **parameter search** and **launch multiple runs at once**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how to iteratively evaluate and tune a Retrieval-Augmented Generation (RAG) system using [Fondant](https://fondant.ai).\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Set up a [Weaviate](https://weaviate.io/platform) vector store\n",
    "2. Define a parameter set to test\n",
    "3. Run a Fondant pipeline with those parameters to index our documents into the vector store\n",
    "4. Run a Fondant pipeline with those parameters to evaluate the performance\n",
    "5. Inspect the evaluation results and data between each processing step\n",
    "6. Repeat step 2 - 5 until we're happy with the results\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../art/iteration.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [**Fondant**](https://fondant.ai), **a hub and framework for easy and shareable data processing**, which has the following advantages for RAG evaluation:\n",
    "\n",
    "- **Speed**\n",
    "    - Reusable RAG components from the [Fondant Hub](https://fondant.ai/en/latest/components/hub/) to quickly build RAG pipelines\n",
    "    - [Pipeline caching](https://fondant.ai/en/latest/caching/) to speed up subsequent runs\n",
    "    - Parallel processing out of the box to speed up processing of large datasets\n",
    "- **Ease-of-use**\n",
    "    - Change parameters and swap [components](https://fondant.ai/en/latest/components/hub/) by changing only a few lines of code\n",
    "    - Create your own [custom components](https://fondant.ai/en/latest/components/custom_component/) (e.g. with different chunking strategies) and plug them into your pipeline\n",
    "    - Reuse your processing components in different pipelines and share them with the [community](https://discord.gg/HnTdWhydGp)\n",
    "- **Production-readiness**\n",
    "    - Full data lineage and a [data explorer](https://fondant.ai/en/latest/data_explorer/) to check the evolution of data after each step\n",
    "    - Ready to deploy to (managed) platforms such as _Vertex, SageMaker and Kubeflow_\n",
    " \n",
    "Share your experiences or let us know how we can improve through our [**Discord**](https://discord.gg/HnTdWhydGp) or on [**GitHub**](https://github.com/ml6team/fondant). And of course feel free to give us a [**star ‚≠ê**](https://github.com/ml6team/fondant-usecase-RAG) if you like what we are doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è This section checks the prerequisites of your environment. Read any errors or warnings carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure a **Python between version 3.8 and 3.10** is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info < (3, 8, 0) or sys.version_info >= (3, 11, 0):\n",
    "    raise Exception(f\"A Python version between 3.8 and 3.10 is required. You are running {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if **docker compose** is installed and the **docker daemon** is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install **Fondant** framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r ../requirements.txt --disable-pip-version-check && echo \"Success\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that **logs** are displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.info(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if GPU is available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    subprocess.check_output('nvidia-smi')\n",
    "    logging.info(\"Found GPU, using it!\")\n",
    "    number_of_accelerators = 1\n",
    "    accelerator_name = \"GPU\"\n",
    "except Exception:\n",
    "    logging.warning(\"We recommend to run this pipeline on a GPU, but none could be found, using CPU instead\")\n",
    "    number_of_accelerators = None\n",
    "    accelerator_name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Spin up the Weaviate vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è For **Apple M1/M2** chip users:\n",
    "> \n",
    "> - In Docker Desktop Dashboard `Settings -> Features in development`, make sure to **un**check `Use containerd` for pulling and storing images. More info [here](https://docs.docker.com/desktop/settings/mac/#beta-features)\n",
    "> - Make sure that Docker uses linux/amd64 platform and not arm64 (cell below should take care of that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DOCKER_DEFAULT_PLATFORM\"]=\"linux/amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run **Weaviate** with Docker compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f weaviate/docker-compose.yaml up --detach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have **Weaviate client v3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"weaviate-client==3.*\" --disable-pip-version-check && echo \"Weaviate client installed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the vectorDB is running and accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import weaviate\n",
    "\n",
    "try:\n",
    "    local_weaviate_client = weaviate.Client(\"http://localhost:8081\")\n",
    "    logging.info(\"Connected to Weaviate instance\")\n",
    "except weaviate.WeaviateStartUpError:\n",
    "    logging.error(\"Cannot connect to weaviate instance, is it running?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pipeline_index.py` processes text data and loads it into the vector database\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../art/indexing_ltr.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "- [**Load data**](https://github.com/ml6team/fondant/tree/main/components/load_from_parquet): loads data from the Hugging Face Hub\n",
    "- [**Chunk data**](https://github.com/ml6team/fondant/tree/main/components/chunk_text): divides the text into sections of a certain size and with a certain overlap\n",
    "- [**Embed chunks**](https://github.com/ml6team/fondant/tree/main/components/embed_text): embeds each chunk as a vector.  \n",
    "- [**Index vector store**](https://github.com/ml6team/fondant/tree/main/components/index_weaviate): writes data and embeddings to the vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° This notebook defaults to the first 1000 rows of the [wikitext](https://huggingface.co/datasets/wikitext) dataset for demonstration purposes, but you can load your own dataset using one the other load components available on the [**Fondant Hub**](https://fondant.ai/en/latest/components/hub/#component-hub) or by creating your own [**custom load component**](https://fondant.ai/en/latest/guides/implement_custom_components/). Keep in mind that changing the dataset implies that you also need to change the evaluation dataset used in the evaluation pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the indexing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the arguments below if you want to run the pipeline with different parameters. For more information on the possible values, check out the above links to the component documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline_index\n",
    "import utils\n",
    "\n",
    "# Path where data and artifacts will be stored\n",
    "BASE_PATH = \"./data\"\n",
    "utils.create_directory_if_not_exists(BASE_PATH)\n",
    "\n",
    "# Parameters shared between indexing and evaluation pipeline\n",
    "shared_args = {\n",
    "    \"base_path\": BASE_PATH,\n",
    "    \"embed_model_provider\": \"huggingface\",\n",
    "    \"embed_model\": \"all-MiniLM-L6-v2\",\n",
    "    \"embed_api_key\": {},\n",
    "    \"weaviate_url\": f\"http://{utils.get_host_ip()}:8081\",\n",
    "    \"weaviate_class\": \"Pipeline1\", # Capitalized, avoid special characters (_, =, -, etc.)\n",
    "}\n",
    "\n",
    "# Parameters for the indexing pipeline\n",
    "indexing_args = {\n",
    "    \"n_rows_to_load\": 1000,\n",
    "    \"chunk_size\": 1024,\n",
    "    \"chunk_overlap\": 8,\n",
    "}\n",
    "\n",
    "# Parameters for the GPU resources\n",
    "resources_args = {\n",
    "    \"number_of_accelerators\": number_of_accelerators,\n",
    "    \"accelerator_name\": accelerator_name,\n",
    "}\n",
    "\n",
    "indexing_pipeline = pipeline_index.create_pipeline(**shared_args, **indexing_args, **resources_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the indexing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° The first time you run a pipeline, you need to **download a docker image for each component** which may take a minute.\n",
    "\n",
    "> üí° Use a **GPU** or an external API to speed up the embedding step\n",
    "\n",
    "> üí° Steps that have been processed before are **cached** and will be skipped in subsequent runs which speeds up processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.pipeline.runner import DockerRunner\n",
    "\n",
    "runner = DockerRunner()\n",
    "runner.run(indexing_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pipeline_eval.py` evaluates retrieval performance using the questions provided in your test dataset\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"../art/evaluation_ltr.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "- [**Load eval data**](https://github.com/ml6team/fondant/tree/main/components/load_from_csv): loads the evaluation dataset (questions) from a csv file\n",
    "- [**Embed questons**](https://github.com/ml6team/fondant/tree/main/components/embed_text): embeds each chunk as a vector\n",
    "- [**Query vector store**](https://github.com/ml6team/fondant/tree/main/components/retrieve_from_weaviate): retrieves the most relevant chunks for each question from the vector store\n",
    "- [**Evaluate**](https://github.com/ml6team/fondant/tree/0.8.0/components/evaluate_ragas): evaluates the retrieved chunks for each question, e.g. using [RAGAS](https://docs.ragas.io/en/latest/index.html)\n",
    "- [**Aggregate**](https://github.com/ml6team/fondant-usecase-RAG/tree/main/src/components/aggregate_eval_results): calculates aggregated results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è If you want to use an **OpenAI** model for evaluation you will need an [API key](https://platform.openai.com/docs/quickstart) (see TODO below)\n",
    "\n",
    "Change the arguments below if you want to run the pipeline with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pipeline_eval\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "evaluation_args = {\n",
    "    \"retrieval_top_k\": 2,\n",
    "    \"evaluation_module\": \"langchain.chat_models\",\n",
    "    \"evaluation_llm\": \"ChatOpenAI\",\n",
    "    \"evaluation_llm_kwargs\": {\n",
    "                              \"openai_api_key\": os.environ[\"OPENAI_API_KEY\"],   # TODO: Update with your key or use a different model\n",
    "                              \"model_name\" : \"gpt-3.5-turbo\"\n",
    "    },\n",
    "    \"evaluation_metrics\": [\"context_precision\", \"context_relevancy\"]\n",
    "}\n",
    "\n",
    "evaluation_pipeline = pipeline_eval.create_pipeline(**shared_args, **evaluation_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the evaluation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = DockerRunner()\n",
    "extra_volumes = [str(os.path.join(os.path.abspath('.'), \"evaluation_datasets\")) + \":/evaldata\"]\n",
    "runner.run(evaluation_pipeline, extra_volumes=extra_volumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° Check out our [**advanced notebook**](./parameter_search.ipynb) if you want to perform **parameter search** and **launch multiple runs at once**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.get_metrics_latest_run(base_path=BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check your data and results at each step in the pipelines using the **Fondant data explorer**. The first time you run the data explorer, you need to download the docker image which may take a minute. Then you can access the data explorer at: **http://localhost:8501/**\n",
    "\n",
    "Enjoy the exploration! üç´ \n",
    "\n",
    "Press the ‚óºÔ∏è in the notebook toolbar to **stop the explorer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.explore import run_explorer_app\n",
    "\n",
    "run_explorer_app(base_path=BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After your pipeline run successfully, you can **clean up** your environment and stop the weaviate database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f weaviate/docker-compose.yaml down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please share your experience or **let us know how we can improve** through our \n",
    "* [**Discord**](https://discord.gg/HnTdWhydGp) \n",
    "* [**GitHub**](https://github.com/ml6team/fondant)\n",
    "\n",
    "And of course feel free to give us a [**star** ‚≠ê](https://github.com/ml6team/fondant) if you like what we are doing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
