{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🍫 Building a RAG indexing pipeline with Fondant\n",
    "\n",
    "> ⚠️ Please note that this notebook **is not** compatible with **Google Colab**. To complete the tutorial, you must \n",
    "> initiate Docker containers. Starting Docker containers within Google Colab is not supported.\n",
    "\n",
    "This repository demonstrates a Fondant data pipeline that ingests text\n",
    "data into a vector database. The pipeline uses four reusable Fondant components.  \n",
    "Additionally, we provide a Docker Compose setup for Weaviate, enabling local testing and\n",
    "development.\n",
    "\n",
    "### Pipeline overview\n",
    "\n",
    "The primary goal of this sample is to showcase how you can use a Fondant pipeline and reusable\n",
    "components to load, chunk and embed text, as well as ingest the text embeddings to a vector\n",
    "database.\n",
    "Pipeline Steps:\n",
    "\n",
    "- [Data Loading](https://github.com/ml6team/fondant/tree/main/components/load_from_parquet): The\n",
    "  pipeline begins by loading text data from a Parquet file, which serves as the\n",
    "  source for subsequent processing. For the minimal example we are using a dataset from Huggingface.\n",
    "- [Text Chunking](https://github.com/ml6team/fondant/tree/main/components/chunk_text): Text data is\n",
    "  chunked into manageable sections to prepare it for embedding. This\n",
    "  step\n",
    "  is crucial for performant RAG systems.\n",
    "- [Text Embedding](https://github.com/ml6team/fondant/tree/main/components/embed_text): We are using\n",
    "  a small HuggingFace model for the generation of text embeddings.\n",
    "  The `embed_text` component easily allows the usage of different models as well.\n",
    "- [Write to Weaviate](https://github.com/ml6team/fondant/tree/main/components/index_weaviate): The\n",
    "  final step of the pipeline involves writing the embedded text data to\n",
    "  a Weaviate database.\n",
    "\n",
    "## Environment\n",
    "### This section checks the prerequisites of your environment. Read any errors or warnings carefully. \n",
    "\n",
    "**Ensure a Python between version 3.8 and 3.10 is available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info < (3, 8, 0) or sys.version_info >= (3, 11, 0):\n",
    "    raise Exception(f\"A Python version between 3.8 and 3.10 is required. You are running {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if docker compose is installed and the docker daemon is running**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose version\n",
    "!docker ps && echo \"Docker running\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if GPU is available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    subprocess.check_output('nvidia-smi')\n",
    "    logging.info(\"Found GPU, using it!\")\n",
    "    number_of_accelerators = 1\n",
    "    accelerator_name = \"GPU\"\n",
    "except Exception:\n",
    "    logging.warning(\"We recommend to run this pipeline on a GPU, but none could be found, using CPU instead\")\n",
    "    number_of_accelerators = None\n",
    "    accelerator_name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Fondant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt\n",
    "# TODO: remove after component inspection PR is merged \n",
    "!pip3 install \"fondant[component,aws,azure,gcp]@git+https://github.com/ml6team/fondant@f0326c09c1c681a5d275605fe57eeb65918ce6c7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the pipeline\n",
    "\n",
    "First of all, we need to initialize the pipeline, which includes specifying a name for your pipeline, providing a description, and setting a base_path. The base_path is used to store the pipeline artifacts and data generated by the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from fondant.pipeline import Pipeline, Resources\n",
    "\n",
    "BASE_PATH = \"./data\"\n",
    "Path(BASE_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=\"ingestion-pipeline\",  # Add a unique pipeline name to easily track your progress and data\n",
    "    description=\"Pipeline to prepare and process data for building a RAG solution\",\n",
    "    base_path=BASE_PATH, # The demo pipelines uses a local directory to store the data.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, we will utilize a dataset available on Hugging Face. As such, we will use a reusable Fondant component `load_from_hf_hub`. Note that the `load_from_hf_hub` component does not define a fixed schema for the data it produces, which means we need to provide hits ourselves with the `produces` argument. It takes a mapping from field names to `pyarrow` types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "text = pipeline.read(\n",
    "    \"components/load_from_hf_hub\",\n",
    "    arguments={\n",
    "        # Add arguments\n",
    "        \"dataset_name\": \"wikitext@~parquet\",\n",
    "        \"n_rows_to_load\": 1000,\n",
    "    },\n",
    "    produces={\n",
    "        \"text\": pa.string()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import typing as t \n",
    "\n",
    "from fondant.component import PandasTransformComponent\n",
    "from fondant.pipeline import lightweight_component\n",
    "import logging\n",
    "import typing as t \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "#TODO: Move all imports defined within functions under the class definition after https://github.com/ml6team/fondant/pull/835 is merged \n",
    "@lightweight_component(\n",
    "    consumes={\"text\":pa.string()},\n",
    "    produces={\"text\":pa.string(), \"original_document_id\":pa.string()},\n",
    "    extra_requires=[\"langchain==0.0.329\"]\n",
    ")\n",
    "class ChunkTextComponent(PandasTransformComponent):\n",
    "    \"\"\"Component that chunks text into smaller segments.\n",
    "    More information about the different chunking strategies can be here:\n",
    "      - https://python.langchain.com/docs/modules/data_connection/document_transformers/\n",
    "      - https://www.pinecone.io/learn/chunking-strategies/.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        chunk_size: int,\n",
    "        chunk_overlap: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            chunk_size: the chunk size \n",
    "            chunk_overlap: the overlap between chunks\n",
    "        \"\"\"\n",
    "        import logging\n",
    "        import typing as t \n",
    "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.chunker = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "    def chunk_text(self, row) -> t.List[t.Tuple]:\n",
    "        # Multi-index df has id under the name attribute\n",
    "        doc_id = row.name\n",
    "        text_data = row[\"text\"]\n",
    "        docs = self.chunker.create_documents([text_data])\n",
    "\n",
    "        return [\n",
    "            (doc_id, f\"{doc_id}_{chunk_id}\", chunk.page_content)\n",
    "            for chunk_id, chunk in enumerate(docs)\n",
    "        ]\n",
    "\n",
    "    def transform(self, dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "        import itertools\n",
    "        \n",
    "        self.logger.info(f\"Chunking {len(dataframe)} documents...\")\n",
    "\n",
    "        results = dataframe.apply(\n",
    "            self.chunk_text,\n",
    "            axis=1,\n",
    "        ).to_list()\n",
    "\n",
    "        # Flatten results\n",
    "        results = list(itertools.chain.from_iterable(results))\n",
    "\n",
    "        # Turn into dataframes\n",
    "        results_df = pd.DataFrame(\n",
    "            results,\n",
    "            columns=[\"original_document_id\", \"id\", \"text\"],\n",
    "        )\n",
    "        results_df = results_df.set_index(\"id\")\n",
    "\n",
    "        return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method doesn't execute the component yet, but adds it to the execution graph of the pipeline, and returns a lazy `Dataset` instance. We can now chain additional components from the [Fondant Hub](https://fondant.ai/en/latest/components/hub/) using the `Dataset.apply()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# TODO: remove /components after using a stable release \n",
    "\n",
    "chunks = text.apply(\n",
    "    ChunkTextComponent,\n",
    "    arguments={\n",
    "        \"chunk_size\": 512, \"chunk_overlap\": 32\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "embeddings = chunks.apply(\n",
    "    \"components/embed_text\",\n",
    "    arguments={\n",
    "        \"model_provider\": \"huggingface\",\n",
    "        \"model\": \"all-MiniLM-L6-v2\"\n",
    "    },\n",
    "    resources=Resources(\n",
    "        accelerator_number=number_of_accelerators,\n",
    "        accelerator_name=accelerator_name,\n",
    "    ),\n",
    "    cluster_type=\"local\" if number_of_accelerators is not None else \"default\",\n",
    ")\n",
    "\n",
    "embeddings.write(\n",
    "    \"components/index_weaviate\",\n",
    "    arguments={\n",
    "        \"weaviate_url\": f\"http://{utils.get_host_ip()}:8081\",\n",
    "        \"class_name\": \"index\",\n",
    "    },\n",
    "    cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline now looks as follows:\n",
    "\n",
    "`read_from_hf_hub` -> `chunk_text` -> `embed_text` -> `index_weaviate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the pipeline\n",
    "\n",
    "The pipeline will load and process text data, then ingest the processed data into a vector database. Before executing the pipeline, we need to start the Weaviate database. Otherwise the pipeline execution will fail.\n",
    "\n",
    "To do this, we can utilize the Docker setup provided in the `weaviate` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using a MacBook with a M1 processor you have to make sure to set the docker default platform to linux/amd64\n",
    "import os\n",
    "os.environ[\"DOCKER_DEFAULT_PLATFORM\"]=\"linux/amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f weaviate_service/docker-compose.yaml up --detach --quiet-pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can execute our pipeline. \n",
    "Fondant provides multiple runners to run our pipeline:\n",
    "\n",
    "- A Docker runner for local execution\n",
    "- A Vertex AI runner for managed execution on Google Cloud\n",
    "- A Sagemaker runner for managed execution on AWS\n",
    "- A Kubeflow Pipelines runner for execution anywhere\n",
    "Here we will use the DockerRunner for local execution, which utilizes docker-compose under the hood.\n",
    "\n",
    "The runner will download the reusable components from the component hub. Afterwards, you will see the components execute one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.pipeline.runner import DockerRunner\n",
    "\n",
    "DockerRunner().run(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset\n",
    "\n",
    "You can also explore the dataset using the fondant explorer, this enables you to visualize your output dataset at each component step. It might take a while to start the first time as it needs to download the explorer docker image first. You can browse at \n",
    "http://localhost:8501/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.explore import run_explorer_app\n",
    "\n",
    "run_explorer_app(base_path=BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop the Explore, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.explore import stop_explorer_app\n",
    "\n",
    "stop_explorer_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your own component\n",
    "\n",
    "Certainly, you can create your own custom components and use them in the pipeline. Let's consider building a component that cleans our text articles. For demo purpose we will implement a component thats removes all empty lines.\n",
    "\n",
    "We will implement this component as a lightweight component, checkout our [guide](https://fondant.ai/en/latest/components/lightweight_components/) on lightweight components for more info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lightweight_component(\n",
    "    consumes={\"text\":pa.string()},\n",
    "    produces={\"text\":pa.string()},\n",
    ")\n",
    "class TextCleaningComponent(PandasTransformComponent):\n",
    "    def __init__(\n",
    "        self\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            chunk_size: the chunk size \n",
    "            chunk_overlap: the overlap between chunks\n",
    "        \"\"\"\n",
    "        import logging\n",
    "        import typing as t \n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def remove_empty_lines(self, text):\n",
    "        lines = text.split(\"\\n\")\n",
    "        non_empty_lines = [line.strip() for line in lines if line.strip()]\n",
    "        return 1\n",
    "\n",
    "    def transform(self, dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "        dataframe[\"text\"] = dataframe[\"text\"].apply(\n",
    "            self.remove_empty_lines\n",
    "        )\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the new component to the pipeline\n",
    "\n",
    "Now we can add the new component to the pipeline with the `Dataset.apply` function. We just specify the reference to the component class containing the custom component instead of the name of the reusable component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from fondant.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# TODO: remove /components after using a stable release \n",
    "pipeline = Pipeline(\n",
    "    name=\"ingestion-pipeline\",\n",
    "    description=\"Pipeline to prepare and process data for building a RAG solution\",\n",
    "    base_path=BASE_PATH,  # The demo pipelines uses a local directory to store the data.\n",
    ")\n",
    "\n",
    "text = pipeline.read(\n",
    "    \"components/load_from_hf_hub\",\n",
    "    arguments={\n",
    "        \"dataset_name\": \"wikitext@~parquet\",\n",
    "        \"n_rows_to_load\": 1000,\n",
    "    },\n",
    "    produces={\n",
    "        \"text\": pa.string()\n",
    "    }\n",
    ")\n",
    "\n",
    "cleaned_text = text.apply(\n",
    "    TextCleaningComponent\n",
    ")\n",
    "\n",
    "chunks = cleaned_text.apply(\n",
    "    \"components/chunk_text\",\n",
    "    arguments={\n",
    "        \"chunk_size\": 512,\n",
    "        \"chunk_overlap\": 32,\n",
    "    },\n",
    ")\n",
    "\n",
    "embeddings = chunks.apply(\n",
    "    \"components/embed_text\",\n",
    "    arguments={\n",
    "        \"model_provider\": \"huggingface\",\n",
    "        \"model\": \"all-MiniLM-L6-v2\",\n",
    "    },\n",
    ")\n",
    "\n",
    "embeddings.write(\n",
    "    \"components/index_weaviate\",\n",
    "    arguments={\n",
    "        \"weaviate_url\": f\"http://{utils.get_host_ip()}:8081\",\n",
    "        \"class_name\": \"index\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you now run your pipeline, the new changes will be picked up and Fondant will automatically re-build the component with the changes included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DockerRunner().run(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check the logs, you will see that components executed with the same parameters which enables faster pipeline iteration.\n",
    "\n",
    "If you restart the Explorer, you'll see that you can now select a second pipeline and inspect your new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_explorer_app(base_path=BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up your environment\n",
    "\n",
    "After your pipeline run successfully, you should clean up your environment and stop the weaviate database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f weaviate/docker-compose.yaml down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_explorer_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling up\n",
    "If you're happy with your dataset, it's time to scale up. Check [our documentation](https://fondant.ai/en/latest/pipeline/#compiling-and-running-a-pipeline) for more information about the available runners.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
