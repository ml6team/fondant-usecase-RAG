{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if docker compose is installed and the docker daemon is running**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation\n",
    "!docker compose version >/dev/null\n",
    "!docker info >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Fondant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initiate the weaviate vectorDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using a MacBook with a M1 processor you have to make sure to set the docker default platform to linux/amd64\n",
    "import os\n",
    "os.environ[\"DOCKER_DEFAULT_PLATFORM\"]=\"linux/amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Weaviate with Docker compose\n",
    "!docker compose -f weaviate/docker-compose.yaml up --detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the vectorDB is running and accessible\n",
    "import weaviate\n",
    "local_weaviate_client = weaviate.Client(\"http://localhost:8080\")\n",
    "local_weaviate_client.schema.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-Search (still developing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the pipelines creator and the pipeline runner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.pipeline.runner import DockerRunner\n",
    "import pipeline_index, pipeline_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the functions to run the different pipelines and output the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Host IP address\n",
    "import socket\n",
    "\n",
    "def get_host_ip():\n",
    "    try:\n",
    "        # This step is done to get the local machine's IP address\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        s.connect((\"8.8.8.8\", 80))\n",
    "        host_ip = s.getsockname()[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error while retrieving host IP address: {e}\")\n",
    "        host_ip = None\n",
    "    finally:\n",
    "        s.close()\n",
    "\n",
    "    return host_ip\n",
    "\n",
    "# Example usage\n",
    "host_ip = get_host_ip()\n",
    "print(f\"Host IP address: {host_ip}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index pipeline runner\n",
    "def run_indexing_pipeline(runner, index_pipeline, host_ip, weaviate_class_name):\n",
    "    runner.run(index_pipeline)\n",
    "    docker_weaviate_client = weaviate.Client(f\"http://{host_ip}:8080\")\n",
    "    return docker_weaviate_client.schema.get(weaviate_class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval pipeline runner\n",
    "def run_evaluation_pipeline(runner, eval_pipeline, extra_volumes):\n",
    "    runner.run(input=eval_pipeline, extra_volumes=extra_volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read latest chosen component\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_latest_data(base_path: str, pipeline_name: str, component_name: str):\n",
    "    # Specify the path to the 'data' directory\n",
    "    data_directory = f\"{base_path}/{pipeline_name}\"\n",
    "\n",
    "    # Get a list of all subdirectories in the 'data' directory\n",
    "    subdirectories = [\n",
    "        d\n",
    "        for d in os.listdir(data_directory)\n",
    "        if os.path.isdir(os.path.join(data_directory, d))\n",
    "    ]\n",
    "\n",
    "    # keep pipeline directories\n",
    "    valid_entries = [\n",
    "        entry for entry in subdirectories if entry.startswith(pipeline_name)\n",
    "    ]\n",
    "    # keep pipeline folders containing a parquet file in the component folder\n",
    "    valid_entries = [\n",
    "        folder\n",
    "        for folder in valid_entries\n",
    "        if has_parquet_file(data_directory, folder, component_name)\n",
    "    ]\n",
    "    # keep the latest folder\n",
    "    latest_folder = sorted(valid_entries, key=extract_timestamp, reverse=True)[0]\n",
    "\n",
    "    # If a valid folder is found, proceed to read all Parquet files in the component folder\n",
    "    if latest_folder:\n",
    "        # Find the path to the component folder\n",
    "        component_folder = os.path.join(data_directory, latest_folder, component_name)\n",
    "\n",
    "        # Get a list of all Parquet files in the component folder\n",
    "        parquet_files = [\n",
    "            f for f in os.listdir(component_folder) if f.endswith(\".parquet\")\n",
    "        ]\n",
    "\n",
    "        if parquet_files:\n",
    "            # Read all Parquet files and concatenate them into a single DataFrame\n",
    "            dfs = [\n",
    "                pd.read_parquet(os.path.join(component_folder, file))\n",
    "                for file in parquet_files\n",
    "            ]\n",
    "            return pd.concat(dfs, ignore_index=True)\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def has_parquet_file(data_directory, entry, component_name):\n",
    "    component_folder = os.path.join(data_directory, entry, component_name)\n",
    "    # Check if the component exists\n",
    "    if not os.path.exists(component_folder) or not os.path.isdir(component_folder):\n",
    "        return False\n",
    "    parquet_files = [\n",
    "        file for file in os.listdir(component_folder) if file.endswith(\".parquet\")\n",
    "    ]\n",
    "    return bool(parquet_files)\n",
    "\n",
    "\n",
    "def extract_timestamp(folder_name):\n",
    "    # Extract the timestamp part from the folder name\n",
    "    timestamp_str = folder_name.split(\"-\")[-1]\n",
    "    # Convert the timestamp string to a datetime object\n",
    "    return datetime.strptime(timestamp_str, \"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the Grid-Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid-search/iterative loop\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Define evaluation dataset to load (csv file with a \"question\" column)\n",
    "local_folder_absolute_path = \"fondant-usecase-RAG/src/local_file\" #TODO Repace with absolute Path\n",
    "extra_volumes = [f\"{local_folder_absolute_path}:/data\"]\n",
    "\n",
    "# Define the values for grid search\n",
    "chunk_sizes = [256, 512]\n",
    "chunk_overlaps = [10, 50]\n",
    "embed_models = [(\"huggingface\",\"all-MiniLM-L6-v2\"), (\"huggingface\", \"BAAI/bge-small-en\")]\n",
    "top_ks = [2, 5]\n",
    "\n",
    "# Fixed parameters\n",
    "fixed_args = {\n",
    "    \"pipeline_dir\":\"./data-dir\",\n",
    "    \"weaviate_url\":f\"http://{host_ip}:8080\", # IP address \n",
    "}\n",
    "fixed_index_args = {\n",
    "    \"hf_dataset_name\":\"wikitext@~parquet\",\n",
    "    \"data_column_name\":\"text\",\n",
    "    \"n_rows_to_load\":1000,\n",
    "}\n",
    "fixed_eval_args = {\n",
    "    \"csv_dataset_uri\":\"/data/wikitext_1000_q.csv\", #make sure it is the same as mounted file\n",
    "    \"csv_column_separator\":\";\",\n",
    "    \"question_column_name\":\"question\",\n",
    "    \"module\": \"langchain.llms\",\n",
    "    \"llm_name\":\"OpenAI\",\n",
    "    \"llm_kwargs\":{\"openai_api_key\": \"\"}, #TODO Specify your key in you're using OpenAI\n",
    "    \"metrics\":[\"context_precision\", \"context_relevancy\"]\n",
    "}\n",
    "\n",
    "# Define pipeline runner\n",
    "runner = DockerRunner()\n",
    "\n",
    "# Results dictionary to store results for each iteration\n",
    "results_dict = {}\n",
    "\n",
    "# Perform grid search\n",
    "indexes = []\n",
    "for i, (chunk_size, chunk_overlap, embed_model) in enumerate(itertools.product(chunk_sizes, chunk_overlaps, embed_models), start=1):\n",
    "    index_config_class_name = f\"IndexConfig{i}\"\n",
    "    logging.info(f\"Running indexing for {index_config_class_name} with chunk_size={chunk_size}, chunk_overlap={chunk_overlap}, embed_model={embed_model}\")\n",
    "\n",
    "    # Store Indexing configuration\n",
    "    index_dict = {}\n",
    "    index_dict[\"index_name\"] = index_config_class_name\n",
    "    index_dict[\"chunk_size\"] = chunk_size\n",
    "    index_dict[\"chunk_overlap\"] = chunk_overlap\n",
    "    index_dict[\"embed_model\"] = embed_model\n",
    "    indexes.append(index_dict)\n",
    "    \n",
    "    # Create and Run the indexing pipeline\n",
    "    indexing_pipeline = pipeline_index.create_pipeline(\n",
    "        **fixed_args,\n",
    "        **fixed_index_args,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        embed_model_provider=embed_model[0],\n",
    "        embed_model=embed_model[1],\n",
    "        weaviate_class_name=index_config_class_name\n",
    "    )\n",
    "    run_indexing_pipeline(\n",
    "        runner=runner,\n",
    "        index_pipeline=indexing_pipeline,\n",
    "        host_ip=host_ip,\n",
    "        weaviate_class_name=index_config_class_name\n",
    "    )\n",
    "    \n",
    "grid_search_results = []\n",
    "for i, (index_dict, top_k) in enumerate(itertools.product(indexes, top_ks), start=1):\n",
    "    rag_config_name = f\"RAGConfig{i}\"\n",
    "    logging.info(f\"Running evaluation for {rag_config_name} with {index_dict['index_name']} and {top_k} retrieved chunks\")\n",
    "\n",
    "    # Store RAG pipeline configuration\n",
    "    results_dict = {}\n",
    "    results_dict[\"rag_config_name\"] = rag_config_name\n",
    "    results_dict.update(index_dict)\n",
    "    results_dict[\"top_k\"] = top_k\n",
    "\n",
    "    # Create and Run the evaluation pipeline\n",
    "    evaluation_pipeline = pipeline_eval.create_pipeline(\n",
    "        **fixed_args,\n",
    "        **fixed_eval_args,\n",
    "        embed_model_provider=index_dict[\"embed_model\"][0],\n",
    "        embed_model=index_dict[\"embed_model\"][1],\n",
    "        weaviate_class_name=index_dict[\"index_name\"],\n",
    "        top_k=top_k,\n",
    "    )\n",
    "    run_evaluation_pipeline(\n",
    "        runner=runner,\n",
    "        eval_pipeline=evaluation_pipeline,\n",
    "        extra_volumes=extra_volumes\n",
    "    )\n",
    "\n",
    "    # Save the evaluation results in the dictionary\n",
    "    results_dict[f\"agg_results_{rag_config_name}\"] = read_latest_data(\n",
    "        base_path=\"./data-dir\",\n",
    "        pipeline_name=\"evaluation-pipeline\",\n",
    "        component_name=\"aggregate_eval_results\"\n",
    "    )\n",
    "    grid_search_results.append(results_dict)\n",
    "\n",
    "# Print the results\n",
    "for config, results in results_dict.items():\n",
    "    print(config)\n",
    "    print(results)\n",
    "    print(\"/n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset\n",
    "\n",
    "You can explore your results using the fondant explorer, this enables you to visualize your output dataset at each component step. It might take a while to start the first time as it needs to download the explorer docker image first. \n",
    "\n",
    "Enjoy the exploration! 🍫 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.explore import run_explorer_app\n",
    "\n",
    "run_explorer_app(base_path=fixed_args[\"pipeline_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Latest Evaluated Pipeline Score**\n",
    "\n",
    "You can also read the results for each RAG configuration ran. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for results in grid_search_results:\n",
    "    print(\"Results:\")\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, pd.DataFrame):\n",
    "            # If the value is a DataFrame, display it nicely\n",
    "            print(f\"  {key}:\")\n",
    "            print(value)\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    print(\"\\n\" + \"=\" * 30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up your environment\n",
    "\n",
    "After your pipeline run successfully, you should clean up your environment and stop the weaviate database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f weaviate/docker-compose.yaml down"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
