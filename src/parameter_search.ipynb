{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üç´Tune your RAG data pipeline using parameter search and evaluate its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è This notebook can be run on your local machine or on a virtual machine and requires [Docker Compose](https://docs.docker.com/desktop/).\n",
    "> Please note that it is unfortunately **not compatible with Google Colab** as the latter does not support Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° This notebook allows you to perform parameter search, launch multiple runs and compare performance. Check out our [basic notebook](./evaluation.ipynb) if you want to configure a single run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how to perform parameter search and automatically tune a Retrieval-Augmented Generation (RAG) system using [Fondant](https://fondant.ai).\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Set up an environment and a [Weaviate](https://weaviate.io/platform) Vector Store\n",
    "2. Define the sets of parameters that should be tried\n",
    "3. Run the parameter search which automatically:\n",
    "    * Runs an indexing pipeline for each combination of parameters to be tested\n",
    "    * Runs an evaluation pipeline for each index\n",
    "    * Collects results\n",
    "5. Explore results\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../art/iteration.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [**Fondant**](https://fondant.ai), a hub and framework for easy and shareable data processing, as it has the following advantages for RAG evaluation:\n",
    "\n",
    "- **Speed**\n",
    "    - Leverage reusable RAG components from the [Fondant Hub](https://fondant.ai/en/latest/components/hub/) to quickly build RAG pipelines\n",
    "    - [Pipeline caching](https://fondant.ai/en/latest/caching/) to speed up iteration on subsequent runs\n",
    "    - Parallel processing out of the box to speed up processing of large datasets especially\n",
    "    - Local development with the Docker Compose runner (used in this notebook)\n",
    "- **Ease-of-use**\n",
    "    - Easily adaptable: change parameters and swap [components](https://fondant.ai/en/latest/components/hub/) by changing only a few lines of code\n",
    "    - Easily extendable: create your own [custom components](https://fondant.ai/en/latest/components/custom_component/) (eg. with different chunking strategies) and plug them into your pipeline\n",
    "    - Reusable & shareable: reuse your processing components in different pipelines and share them with the [community](https://discord.gg/HnTdWhydGp)\n",
    "- **Production-readiness**\n",
    "    - Pipeline with dockerized steps ready to deploy to (managed) platforms such as _Vertex, SageMaker and Kubeflow_\n",
    "    - Full data lineage and a [data explorer](https://fondant.ai/en/latest/data_explorer/) to check the evolution of data after each step\n",
    "    - Ready to deploy to (managed) platforms such as _Vertex, SageMaker and Kubeflow_\n",
    " \n",
    "Please share your experiences or let us know how we can improve through our [**Discord**](https://discord.gg/HnTdWhydGp) or on [**GitHub**](https://github.com/ml6team/fondant). And of course feel free to give us a [**star ‚≠ê**](https://github.com/ml6team/fondant-usecase-RAG) if you like what we are doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è This section checks the prerequisites of your environment. Read any errors or warnings carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure a Python between version 3.8 and 3.10 is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info < (3, 8, 0) or sys.version_info >= (3, 11, 0):\n",
    "    raise Exception(f\"A Python version between 3.8 and 3.10 is required. You are running {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if docker compose is installed and the docker daemon is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose version\n",
    "!docker info && echo \"Docker running\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Fondant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r ../requirements.txt --disable-pip-version-check && echo \"Success\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_host_ip, create_directory_if_not_exists, run_parameter_search, get_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spin up the Weaviate vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è For Apple M1/M2 chip users:\n",
    "> \n",
    "> - In Docker Desktop Dashboard `Settings -> Features in development`, make sure to **un**check `Use containerd` for pulling and storing images. More info [here](https://docs.docker.com/desktop/settings/mac/#beta-features)\n",
    "> - Make sure that Docker uses linux/amd64 platform and not arm64 (cell below should take care of that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DOCKER_DEFAULT_PLATFORM\"]=\"linux/amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Weaviate with Docker compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f weaviate/docker-compose.yaml up --detach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have Weaviate client v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"weaviate-client==3.*\" --disable-pip-version-check && echo \"Weaviate client installed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the vectorDB is running and accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import weaviate\n",
    "\n",
    "try:\n",
    "    local_weaviate_client = weaviate.Client(\"http://localhost:8080\")\n",
    "    logging.info(\"Connected to Weaviate instance\")\n",
    "except weaviate.WeaviateStartUpError:\n",
    "    logging.error(\"Cannot connect to weaviate instance, is it running?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter search allows you try out different configurations of pipelines and compare their performance. Currently only grid search (which probes all possible combinations of different parameters) has been implemented but more will be added soon.\n",
    "\n",
    "The first pipeline run is `pipeline_index.py`, which processes text data and loads it into the vector database. It consists of the following steps:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../art/indexing_ltr.png\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "- [**HF Data Loading**](https://github.com/ml6team/fondant/tree/main/components/load_from_parquet): loads data from the Hugging Face Hub.\n",
    "- [**Text Chunking**](https://github.com/ml6team/fondant/tree/main/components/chunk_text): divides the text into sections of a certain size and with a certain overlap\n",
    "- [**Text Embedding**](https://github.com/ml6team/fondant/tree/main/components/embed_text): embeds each chunk as a vector.  \n",
    "  üí° Can use different models / APIs. When using a HuggingFace model (the default)\n",
    "- [**Write to Weaviate**](https://github.com/ml6team/fondant/tree/main/components/index_weaviate): writes data and embeddings to the vector store\n",
    "\n",
    "The second pipeline is `pipeline_eval.py` which evaluates retrieval performance using the questions provided in your test dataset.\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"../art/evaluation_ltr.png\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "- [**CSV Data Loading**](https://github.com/ml6team/fondant/tree/main/components/load_from_csv): loads the evaluation dataset (questions) from a csv file.\n",
    "- [**Text Embedding**](https://github.com/ml6team/fondant/tree/main/components/embed_text): embeds each chunk as a vector.\n",
    "  üí° Can use different models / APIs. When using a HuggingFace model (the default), use a machine with GPU for large datasets.\n",
    "- [**Vector store Retrieval**](https://github.com/ml6team/fondant/tree/main/components/retrieve_from_weaviate): retrieves the most relevant chunks for each question from the vector store.\n",
    "- [**Ragas evaluation**](https://github.com/ml6team/fondant/tree/0.8.0/components/evaluate_ragas): evaluates the retrieved chunks for each question with [RAGAS](https://docs.ragas.io/en/latest/index.html).\n",
    "- [**Aggregate metrics**](https://github.com/ml6team/fondant-usecase-RAG/tree/main/src/components/aggregate_eval_results): Aggregate the results on a pipeline level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° This notebook defaults to the first 1000 rows of the [wikitext](https://huggingface.co/datasets/wikitext) dataset for demonstration purposes, but you can load your own dataset using one the other load components available on the [**Fondant Hub**](https://fondant.ai/en/latest/components/hub/#component-hub) or by creating your own [**custom load component**](https://fondant.ai/en/latest/guides/implement_custom_components/). Keep in mind that changing the dataset implies that you also need to change the [evaluation dataset](evaluation_datasets/wikitext_1000_q.csv) used in the evaluation pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select parameters to search over and values to try\n",
    "\n",
    "- `chunk_sizes`: size of each text chunk, in number of characters ([chunk text](https://github.com/ml6team/fondant/tree/main/components/chunk_text) component)\n",
    "- `chunk_overlaps`: overlap between chunks ([chunk text](https://github.com/ml6team/fondant/tree/main/components/chunk_text) component)\n",
    "- `embed_models`: model used to embed ([embed text](https://github.com/ml6team/fondant/tree/main/components/embed_text) component)\n",
    "- `top_ks`: number of retrieved chunks taken into account for evaluation ([retrieve](https://github.com/ml6team/fondant/tree/main/components/retrieve_from_weaviate) component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter search\n",
    "chunk_sizes = [256]\n",
    "chunk_overlaps = [100,150]\n",
    "embed_models = [(\"huggingface\",\"all-MiniLM-L6-v2\")]\n",
    "top_ks = [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è If you want to use ChatGPT you will need an [OpenAI API key](https://platform.openai.com/docs/quickstart) (see TODO below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation dataset to load (csv file with a \"question\" column)\n",
    "extra_volumes = [str(os.path.join(os.path.abspath('.'), \"evaluation_datasets\")) + \":/data\"]\n",
    "\n",
    "# configurable parameters shared by indexing and evaluation pipeline (further below)\n",
    "host_ip = get_host_ip() #get the host IP address to enable Docker access to Weaviate\n",
    "\n",
    "BASE_PATH = \"./data-dir\"\n",
    "BASE_PATH = create_directory_if_not_exists(BASE_PATH) #create a folder to store the pipeline data if it doesn't exist\n",
    "\n",
    "fixed_args = {\n",
    "    \"base_path\":BASE_PATH,\n",
    "    \"weaviate_url\":f\"http://{host_ip}:8080\", # IP address \n",
    "}\n",
    "fixed_index_args = {\n",
    "    \"n_rows_to_load\":1000,\n",
    "}\n",
    "fixed_eval_args = {\n",
    "    \"csv_dataset_uri\":\"/data/wikitext_1000_q.csv\", #make sure it is the same as mounted file\n",
    "    \"csv_separator\":\";\",\n",
    "    \"evaluation_module\": \"langchain.llms\",\n",
    "    \"evaluation_llm\":\"OpenAI\",\n",
    "    \"evaluation_llm_kwargs\":{\"openai_api_key\": \"\"}, #TODO Specify your key if you're using OpenAI\n",
    "    \"evaluation_metrics\":[\"context_precision\", \"context_relevancy\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° The first time you run a pipeline, you need to download a docker image for each component which may take a minute.\n",
    "\n",
    "> üí° Use a GPU to speed up the embedding step (when not using an external API)\n",
    "\n",
    "> üí° Steps that have been processed before are cached and will be skipped in subsequent runs which speeds up processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_search_results = run_parameter_search(\n",
    "    extra_volumes=extra_volumes,\n",
    "    fixed_args=fixed_args,\n",
    "    fixed_index_args=fixed_index_args,\n",
    "    fixed_eval_args=fixed_eval_args,\n",
    "    chunk_sizes=chunk_sizes,\n",
    "    chunk_overlaps= chunk_overlaps,\n",
    "    embed_models=embed_models,\n",
    "    top_ks=top_ks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance of your runs below. The default evaluation component uses [Ragas](https://github.com/explodinggradients/ragas) and provides the following two performance measures [context precision](https://docs.ragas.io/en/latest/concepts/metrics/context_precision.html) and [context relevancy](https://docs.ragas.io/en/latest/concepts/metrics/context_relevancy.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_selected_results(df):\n",
    "    columns_to_show = ['rag_config_name', 'chunk_size', 'chunk_overlap', 'embed_model', 'top_k', 'context_precision', 'context_relevancy']\n",
    "    results_to_show = df[columns_to_show].sort_values('context_precision', ascending=False).set_index('rag_config_name').head(20)\n",
    "    print(f'Showing top {len(results_to_show)} results')\n",
    "    return results_to_show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all results\n",
    "results_df = get_results(results=parameter_search_results)\n",
    "\n",
    "# selected columns & rows\n",
    "show_selected_results(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure plotly is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"plotly\" --disable-pip-version-check && echo \"Plotly installed successfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def add_embed_model_numerical_column(df):\n",
    "    df['embed_model_numerical'] = pd.factorize(df['embed_model'])[0] + 1\n",
    "    return df\n",
    "\n",
    "def show_legend_embed_models(df):\n",
    "    columns_to_show = ['embed_model','embed_model_numerical']\n",
    "    df = df[columns_to_show].drop_duplicates().set_index('embed_model_numerical')\n",
    "    df.index.name = ''\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column with numerical representation of embedding models\n",
    "results_df = add_embed_model_numerical_column(results_df)\n",
    "\n",
    "# show legend\n",
    "show_legend_embed_models(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "dimensions = ['chunk_size', 'chunk_overlap', 'embed_model_numerical', 'top_k', 'context_precision']\n",
    "fig = px.parallel_coordinates(results_df, color=\"context_precision\",\n",
    "                              dimensions=dimensions,\n",
    "                              color_continuous_scale=px.colors.sequential.Bluered)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check your data and results at each step in the pipelines using the Fondant data explorer. The first time you run the data explorer, you need to download the docker image which may take a minute. Afterwards you can access the data explorer at:\n",
    "\n",
    "**http://localhost:8501/**\n",
    "\n",
    "Enjoy the exploration! üç´ \n",
    "\n",
    "Press the ‚óºÔ∏è in the notebook toolbar to stop the explorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.explore import run_explorer_app\n",
    "\n",
    "run_explorer_app(base_path=fixed_args[\"pipeline_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up your environment\n",
    "\n",
    "After your pipeline run successfully, you should clean up your environment and stop the weaviate database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f weaviate/docker-compose.yaml down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please share your experience or let us know how we can improve through our \n",
    "* [**Discord**](https://discord.gg/HnTdWhydGp) \n",
    "* [**GitHub**](https://github.com/ml6team/fondant)\n",
    "\n",
    "And of course feel free to give us a [**star** ‚≠ê](https://github.com/ml6team/fondant) if you like what we are doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
