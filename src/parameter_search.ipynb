{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üç´ Auto-tune your RAG data pipeline using parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è This notebook can be run on your local machine or on a virtual machine and requires [Docker Compose](https://docs.docker.com/desktop/).\n",
    "> Please note that it is not compatible with Google Colab as the latter does not support Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how to **perform parameter search** and **automatically tune a Retrieval-Augmented Generation (RAG)** system using [Fondant](https://fondant.ai).\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Set up an environment and a [Weaviate](https://weaviate.io/platform) Vector Store\n",
    "2. Define the sets of parameters that should be tried\n",
    "3. Run the parameter search which automatically:\n",
    "    * Runs an indexing pipeline for each combination of parameters to be tested\n",
    "    * Runs an evaluation pipeline for each index\n",
    "    * Collects results\n",
    "5. Explore results\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../art/iteration.png\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [**Fondant**](https://fondant.ai), **a hub and framework for easy and shareable data processing**, which has the following advantages for RAG evaluation:\n",
    "\n",
    "- **Speed**\n",
    "    - Reusable RAG components from the [Fondant Hub](https://fondant.ai/en/latest/components/hub/) to quickly build RAG pipelines\n",
    "    - [Pipeline caching](https://fondant.ai/en/latest/caching/) to speed up subsequent runs\n",
    "    - Parallel processing out of the box to speed up processing of large datasets\n",
    "- **Ease-of-use**\n",
    "    - Change parameters and swap [components](https://fondant.ai/en/latest/components/hub/) by changing only a few lines of code\n",
    "    - Create your own [custom components](https://fondant.ai/en/latest/components/custom_component/) (e.g. with different chunking strategies) and plug them into your pipeline\n",
    "    - Reuse your processing components in different pipelines and share them with the [community](https://discord.gg/HnTdWhydGp)\n",
    "- **Production-readiness**\n",
    "    - Full data lineage and a [data explorer](https://fondant.ai/en/latest/data_explorer/) to check the evolution of data after each step\n",
    "    - Ready to deploy to (managed) platforms such as _Vertex, SageMaker and Kubeflow_\n",
    " \n",
    "Share your experiences or let us know how we can improve through our [**Discord**](https://discord.gg/HnTdWhydGp) or on [**GitHub**](https://github.com/ml6team/fondant). And of course feel free to give us a [**star ‚≠ê**](https://github.com/ml6team/fondant-usecase-RAG) if you like what we are doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è This section checks the prerequisites of your environment. Read any errors or warnings carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure a **Python between version 3.8 and 3.10** is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info < (3, 8, 0) or sys.version_info >= (3, 11, 0):\n",
    "    raise Exception(f\"A Python version between 3.8 and 3.10 is required. You are running {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if **docker compose** is installed and the **docker daemon** is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose version\n",
    "!docker ps && echo \"Docker running\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that **logs** are displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.info(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if **GPU** is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    subprocess.check_output('nvidia-smi')\n",
    "    logging.info(\"Found GPU, using it!\")\n",
    "    number_of_accelerators = 1\n",
    "    accelerator_name = \"GPU\"\n",
    "except Exception:\n",
    "    logging.warning(\"We recommend to run this pipeline on a GPU, but none could be found, using CPU instead\")\n",
    "    number_of_accelerators = None\n",
    "    accelerator_name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install **Fondant** framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r ../requirements.txt --disable-pip-version-check && echo \"Success\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spin up the Weaviate vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è For **Apple M1/M2** chip users:\n",
    "> \n",
    "> - In Docker Desktop Dashboard `Settings -> Features in development`, make sure to **un**check `Use containerd` for pulling and storing images. More info [here](https://docs.docker.com/desktop/settings/mac/#beta-features)\n",
    "> - Make sure that Docker uses linux/amd64 platform and not arm64 (cell below should take care of that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DOCKER_DEFAULT_PLATFORM\"]=\"linux/amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run **Weaviate** with Docker compose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have **Weaviate client v3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f weaviate_service/docker-compose.yaml up --detach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the vectorDB is running and accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import weaviate\n",
    "\n",
    "try:\n",
    "    weaviate_client = weaviate.Client(\"http://localhost:8081\")\n",
    "    logging.info(\"Connected to Weaviate instance\")\n",
    "except weaviate.WeaviateStartUpError:\n",
    "    logging.error(\"Cannot connect to weaviate instance, is it running?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter search allows you **try out different configurations of pipelines and compare their performance**\n",
    "\n",
    "`pipeline_index.py` processes text data and loads it into the vector database\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../art/indexing_ltr.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "- [**Load data**](https://github.com/ml6team/fondant/tree/main/components/load_from_parquet): loads data from the Hugging Face Hub\n",
    "- [**Chunk data**](https://github.com/ml6team/fondant/tree/main/components/chunk_text): divides the text into sections of a certain size and with a certain overlap\n",
    "- [**Embed chunks**](https://github.com/ml6team/fondant/tree/main/components/embed_text): embeds each chunk as a vector, e.g. using [Cohere](https://cohere.com/embeddings)\n",
    "- [**Index vector store**](https://github.com/ml6team/fondant/tree/main/components/index_weaviate): writes data and embeddings to the vector store\n",
    "\n",
    "`pipeline_eval.py` evaluates retrieval performance using the questions provided in your test dataset\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"../art/evaluation_ltr.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "- [**Load eval data**](https://github.com/ml6team/fondant/tree/main/components/load_from_csv): loads the evaluation dataset (questions) from a csv file\n",
    "- [**Embed questons**](https://github.com/ml6team/fondant/tree/main/components/embed_text): embeds each question as a vector, e.g. using [Cohere](https://cohere.com/embeddings)\n",
    "- [**Query vector store**](https://github.com/ml6team/fondant/tree/main/components/retrieve_from_weaviate): retrieves the most relevant chunks for each question from the vector store\n",
    "- [**Evaluate**](https://github.com/ml6team/fondant/tree/0.8.0/components/evaluate_ragas): evaluates the retrieved chunks for each question, e.g. using [RAGAS](https://docs.ragas.io/en/latest/index.html)\n",
    "- [**Aggregate**](https://github.com/ml6team/fondant-usecase-RAG/tree/main/src/components/aggregate_eval_results): calculates aggregated results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° This notebook defaults to the first 1000 rows of the [wikitext](https://huggingface.co/datasets/wikitext) dataset for demonstration purposes, but you can load your own dataset using one the other load components available on the [**Fondant Hub**](https://fondant.ai/en/latest/components/hub/#component-hub) or by creating your own [**custom load component**](https://fondant.ai/en/latest/guides/implement_custom_components/). Keep in mind that changing the dataset implies that you also need to change the [evaluation dataset](evaluation_datasets/wikitext_1000_q.csv) used in the evaluation pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose **parameters** over which to search\n",
    "\n",
    "- `chunk_size`: size of each text chunk, in number of characters ([chunk text](https://github.com/ml6team/fondant/tree/main/components/chunk_text) component)\n",
    "- `chunk_overlap`: overlap between chunks ([chunk text](https://github.com/ml6team/fondant/tree/main/components/chunk_text) component)\n",
    "- `embed_model`: model used to embed ([embed text](https://github.com/ml6team/fondant/tree/main/components/embed_text) component)\n",
    "- `retrieval_top_k`: number of retrieved chunks taken into account for evaluation ([retrieve](https://github.com/ml6team/fondant/tree/main/components/retrieve_from_weaviate) component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a **search method**\n",
    "- [`grid search`](https://en.wikipedia.org/wiki/Hyperparameter_optimization): tries all possible combinations\n",
    "- `progressive search` (recommended): identifies best option per step, much more efficient as complexity increases linearly with number of search options vs. exponentially for grid search (with similar performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchable_index_params = {\n",
    "    'chunk_size' : [128, 256, 384],\n",
    "    'chunk_overlap' : [16, 64, 128],\n",
    "}\n",
    "searchable_shared_params = {\n",
    "    'embed_model' : [(\"huggingface\",\"all-MiniLM-L6-v2\")] # add more as tuples: ,(\"huggingface\", \"BAAI/bge-base-en-v1.5\")\n",
    "}\n",
    "searchable_eval_params = {\n",
    "    'retrieval_top_k' : [2, 4, 8]\n",
    "}\n",
    "\n",
    "evaluation_set_path = \"./evaluation_datasets\"\n",
    "search_method = 'progressive_search' # 'grid_search', 'progressive_search'\n",
    "target_metric = 'context_precision' # relevant for 'smart' methods that use previous results to determine params, e.g. progressive search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è If you want to use an **OpenAI** model for evaluation you will need an [API key](https://platform.openai.com/docs/quickstart) (see TODO below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_host_ip\n",
    "\n",
    "\n",
    "# configurable parameters\n",
    "shared_args = {\n",
    "    \"base_path\" : \"./data\", # where data goes\n",
    "    \"embed_api_key\" : {},\n",
    "    \"weaviate_url\" : f\"http://{get_host_ip()}:8081\"\n",
    "}\n",
    "index_args = {\n",
    "    \"n_rows_to_load\" : 1000,\n",
    "}\n",
    "eval_args = {\n",
    "    \"evaluation_set_filename\" : \"wikitext_1000_q.csv\",\n",
    "    \"evaluation_set_separator\" : \";\",\n",
    "    \"llm_module_name\": \"langchain.chat_models\",\n",
    "    \"llm_class_name\": \"ChatOpenAI\",\n",
    "    \"llm_kwargs\": {\n",
    "      \"openai_api_key\": \"\" ,   # TODO: update with your key or use a different model\n",
    "      \"model_name\" : \"gpt-3.5-turbo\" # choose model, e.g. \"gpt-4\", \"gpt-3.5-turbo\"\n",
    "    },\n",
    "    \"evaluation_metrics\" : [\"context_precision\", \"context_relevancy\"]\n",
    "}\n",
    "\n",
    "# Parameters for the GPU resources\n",
    "resource_args = {\n",
    "    \"number_of_accelerators\": number_of_accelerators,\n",
    "    \"accelerator_name\": accelerator_name,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° The first time you run a pipeline, you need to **download a docker image for each component** which may take a minute.\n",
    "\n",
    "> üí° Use a **GPU** or an external API to speed up the embedding step\n",
    "\n",
    "> üí° Steps that have been processed before are **cached** and will be skipped in subsequent runs which speeds up processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import ParameterSearch\n",
    "from utils import check_weaviate_class_exists\n",
    "\n",
    "mysearch = ParameterSearch(\n",
    "    searchable_index_params = searchable_index_params,\n",
    "    searchable_shared_params = searchable_shared_params,\n",
    "    searchable_eval_params = searchable_eval_params,\n",
    "    shared_args = shared_args,\n",
    "    index_args = index_args,\n",
    "    eval_args = eval_args,\n",
    "    resource_args = resource_args,\n",
    "    search_method = search_method,\n",
    "    target_metric = target_metric,\n",
    "    evaluation_set_path=evaluation_set_path,\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "results = mysearch.run(weaviate_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the performance** of your runs below. The default evaluation component uses [Ragas](https://github.com/explodinggradients/ragas) and provides the following two performance measures [context precision](https://docs.ragas.io/en/latest/concepts/metrics/context_precision.html) and [context relevancy](https://docs.ragas.io/en/latest/concepts/metrics/context_relevancy.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure **Plotly** is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"plotly\" --disable-pip-version-check && echo \"Plotly installed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show legend of **embedding models** used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import add_embed_model_numerical_column, show_legend_embed_models\n",
    "\n",
    "results = add_embed_model_numerical_column(results)\n",
    "show_legend_embed_models(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "dimensions = ['chunk_size', 'chunk_overlap', 'embed_model_numerical', 'retrieval_top_k', 'context_precision']\n",
    "fig = px.parallel_coordinates(results, color=\"context_precision\",\n",
    "                              dimensions=dimensions,\n",
    "                              color_continuous_scale=px.colors.sequential.Bluered)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check your data and results at each step in the pipelines using the **Fondant data explorer**. The first time you run the data explorer, you need to download the docker image which may take a minute. Then you can access the data explorer at: **http://localhost:8501/**\n",
    "\n",
    "Enjoy the exploration! üç´ \n",
    "\n",
    "Press the ‚óºÔ∏è in the notebook toolbar to **stop the explorer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.explore import run_explorer_app\n",
    "\n",
    "run_explorer_app(base_path=shared_args[\"base_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop the Explore, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.explore import stop_explorer_app\n",
    "\n",
    "stop_explorer_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After your pipeline ran successfully, you can **clean up** your environment and stop the weaviate database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f weaviate/docker-compose.yaml down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please share your experience or **let us know how we can improve** through our \n",
    "* [**Discord**](https://discord.gg/HnTdWhydGp) \n",
    "* [**GitHub**](https://github.com/ml6team/fondant)\n",
    "\n",
    "And of course feel free to give us a [**star** ‚≠ê](https://github.com/ml6team/fondant) if you like what we are doing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
