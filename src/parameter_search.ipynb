{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üç´ Auto-tune your RAG data pipeline using parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è This notebook can be run on your local machine or on a virtual machine and requires [Docker Compose](https://docs.docker.com/desktop/).\n",
    "> Please note that it is **not compatible with Google Colab** as the latter does not support Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate **how to perform parameter search and automatically tune a Retrieval-Augmented Generation (RAG)** system using [Fondant](https://fondant.ai).\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Set up an environment and a [Weaviate](https://weaviate.io/platform) Vector Store\n",
    "2. Define the sets of parameters that should be tried\n",
    "3. Run the parameter search which automatically:\n",
    "    * Runs an indexing pipeline for each combination of parameters to be tested\n",
    "    * Runs an evaluation pipeline for each index\n",
    "    * Collects results\n",
    "5. Explore results\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../art/iteration.png\" width=\"900\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [**Fondant**](https://fondant.ai), **a hub and framework for easy and shareable data processing**, which has the following advantages for RAG evaluation:\n",
    "\n",
    "- **Speed**\n",
    "    - Reusable RAG components from the [Fondant Hub](https://fondant.ai/en/latest/components/hub/) to quickly build RAG pipelines\n",
    "    - [Pipeline caching](https://fondant.ai/en/latest/caching/) to speed up subsequent runs\n",
    "    - Parallel processing out of the box to speed up processing of large datasets\n",
    "- **Ease-of-use**\n",
    "    - Change parameters and swap [components](https://fondant.ai/en/latest/components/hub/) by changing only a few lines of code\n",
    "    - Create your own [custom components](https://fondant.ai/en/latest/components/custom_component/) (eg. with different chunking strategies) and plug them into your pipeline\n",
    "    - Reuse your processing components in different pipelines and share them with the [community](https://discord.gg/HnTdWhydGp)\n",
    "- **Production-readiness**\n",
    "    - Full data lineage and a [data explorer](https://fondant.ai/en/latest/data_explorer/) to check the evolution of data after each step\n",
    "    - Ready to deploy to (managed) platforms such as _Vertex, SageMaker and Kubeflow_\n",
    " \n",
    "Share your experiences or let us know how we can improve through our [**Discord**](https://discord.gg/HnTdWhydGp) or on [**GitHub**](https://github.com/ml6team/fondant). And of course feel free to give us a [**star ‚≠ê**](https://github.com/ml6team/fondant-usecase-RAG) if you like what we are doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è This section checks the prerequisites of your environment. Read any errors or warnings carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure a **Python between version 3.8 and 3.10** is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info < (3, 8, 0) or sys.version_info >= (3, 11, 0):\n",
    "    raise Exception(f\"A Python version between 3.8 and 3.10 is required. You are running {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if **docker compose** is installed and the **docker daemon** is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker Compose version v2.23.0-desktop.1\n",
      "Client:\n",
      " Version:    24.0.6\n",
      " Context:    default\n",
      " Debug Mode: false\n",
      " Plugins:\n",
      "  buildx: Docker Buildx (Docker Inc.)\n",
      "    Version:  v0.11.2-desktop.5\n",
      "    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-buildx.exe\n",
      "  compose: Docker Compose (Docker Inc.)\n",
      "    Version:  v2.23.0-desktop.1\n",
      "    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-compose.exe\n",
      "  dev: Docker Dev Environments (Docker Inc.)\n",
      "    Version:  v0.1.0\n",
      "    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-dev.exe\n",
      "  extension: Manages Docker extensions (Docker Inc.)\n",
      "    Version:  v0.2.20\n",
      "    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-extension.exe\n",
      "  init: Creates Docker-related starter files for your project (Docker Inc.)\n",
      "    Version:  v0.1.0-beta.9\n",
      "    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-init.exe\n",
      "  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\n",
      "    Version:  0.6.0\n",
      "    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-sbom.exe\n",
      "  scan: Docker Scan (Docker Inc.)\n",
      "    Version:  v0.26.0\n",
      "    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-scan.exe\n",
      "  scout: Docker Scout (Docker Inc.)\n",
      "    Version:  v1.0.9\n",
      "    Path:     C:\\Program Files\\Docker\\cli-plugins\\docker-scout.exe\n",
      "\n",
      "Server:\n",
      " Containers: 11\n",
      "  Running: 2\n",
      "  Paused: 0\n",
      "  Stopped: 9\n",
      " Images: 48\n",
      " Server Version: 24.0.6\n",
      " Storage Driver: overlay2\n",
      "  Backing Filesystem: extfs\n",
      "  Supports d_type: true\n",
      "  Using metacopy: false\n",
      "  Native Overlay Diff: true\n",
      "  userxattr: false\n",
      " Logging Driver: json-file\n",
      " Cgroup Driver: cgroupfs\n",
      " Cgroup Version: 1\n",
      " Plugins:\n",
      "  Volume: local\n",
      "  Network: bridge host ipvlan macvlan null overlay\n",
      "  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\n",
      " Swarm: inactive\n",
      " Runtimes: io.containerd.runc.v2 runc\n",
      " Default Runtime: runc\n",
      " Init Binary: docker-init\n",
      " containerd version: 8165feabfdfe38c65b599c4993d227328c231fca\n",
      " runc version: v1.1.8-0-g82f18fe\n",
      " init version: de40ad0\n",
      " Security Options:\n",
      "  seccomp\n",
      "   Profile: unconfined\n",
      " Kernel Version: 5.15.133.1-microsoft-standard-WSL2\n",
      " Operating System: Docker Desktop\n",
      " OSType: linux\n",
      " Architecture: x86_64\n",
      " CPUs: 12\n",
      " Total Memory: 7.575GiB\n",
      " Name: xps6\n",
      " ID: 93d3dd69-b910-4972-9bbc-ff16936b4713\n",
      " Docker Root Dir: /var/lib/docker\n",
      " Debug Mode: false\n",
      " HTTP Proxy: http.docker.internal:3128\n",
      " HTTPS Proxy: http.docker.internal:3128\n",
      " No Proxy: hubproxy.docker.internal\n",
      " Experimental: false\n",
      " Insecure Registries:\n",
      "  hubproxy.docker.internal:5555\n",
      "  127.0.0.0/8\n",
      " Live Restore Enabled: false\n",
      "\n",
      "\"Docker running\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: No blkio throttle.read_bps_device support\n",
      "WARNING: No blkio throttle.write_bps_device support\n",
      "WARNING: No blkio throttle.read_iops_device support\n",
      "WARNING: No blkio throttle.write_iops_device support\n",
      "WARNING: daemon is not using the default seccomp profile\n"
     ]
    }
   ],
   "source": [
    "!docker compose version\n",
    "!docker info && echo \"Docker running\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that **logs** are displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.info(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up Fondant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Success\"\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -r ../requirements.txt --disable-pip-version-check && echo \"Success\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spin up the Weaviate vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è For **Apple M1/M2** chip users:\n",
    "> \n",
    "> - In Docker Desktop Dashboard `Settings -> Features in development`, make sure to **un**check `Use containerd` for pulling and storing images. More info [here](https://docs.docker.com/desktop/settings/mac/#beta-features)\n",
    "> - Make sure that Docker uses linux/amd64 platform and not arm64 (cell below should take care of that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DOCKER_DEFAULT_PLATFORM\"]=\"linux/amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run **Weaviate** with Docker compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Container weaviate-weaviate-1  Running\n",
      " Container weaviate-contextionary-1  Running\n"
     ]
    }
   ],
   "source": [
    "!docker compose -f weaviate/docker-compose.yaml up --detach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have **Weaviate client v3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Weaviate client installed successfully\"\n"
     ]
    }
   ],
   "source": [
    "!pip install -q \"weaviate-client==3.*\" --disable-pip-version-check && echo \"Weaviate client installed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the vectorDB is running and accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Connected to Weaviate instance\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import weaviate\n",
    "\n",
    "try:\n",
    "    weaviate_client = weaviate.Client(\"http://localhost:8080\")\n",
    "    logging.info(\"Connected to Weaviate instance\")\n",
    "except weaviate.WeaviateStartUpError:\n",
    "    logging.error(\"Cannot connect to weaviate instance, is it running?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter search allows you **try out different configurations of pipelines and compare their performance**\n",
    "\n",
    "`pipeline_index.py` processes text data and loads it into the vector database\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../art/indexing_ltr.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "- [**Load data**](https://github.com/ml6team/fondant/tree/main/components/load_from_parquet): loads data from the Hugging Face Hub\n",
    "- [**Chunk data**](https://github.com/ml6team/fondant/tree/main/components/chunk_text): divides the text into sections of a certain size and with a certain overlap\n",
    "- [**Embed chunks**](https://github.com/ml6team/fondant/tree/main/components/embed_text): embeds each chunk as a vector.  \n",
    "- [**Index vector store**](https://github.com/ml6team/fondant/tree/main/components/index_weaviate): writes data and embeddings to the vector store\n",
    "\n",
    "`pipeline_eval.py` evaluates retrieval performance using the questions provided in your test dataset\n",
    "\n",
    "<div align=center>\n",
    "<img src=\"../art/evaluation_ltr.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "- [**Load eval data**](https://github.com/ml6team/fondant/tree/main/components/load_from_csv): loads the evaluation dataset (questions) from a csv file\n",
    "- [**Embed questons**](https://github.com/ml6team/fondant/tree/main/components/embed_text): embeds each chunk as a vector\n",
    "- [**Query vector store**](https://github.com/ml6team/fondant/tree/main/components/retrieve_from_weaviate): retrieves the most relevant chunks for each question from the vector store\n",
    "- [**Evaluate**](https://github.com/ml6team/fondant/tree/0.8.0/components/evaluate_ragas): evaluates the retrieved chunks for each question, e.g. using [RAGAS](https://docs.ragas.io/en/latest/index.html)\n",
    "- [**Aggregate**](https://github.com/ml6team/fondant-usecase-RAG/tree/main/src/components/aggregate_eval_results): calculates aggregated results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° This notebook defaults to the first 1000 rows of the [wikitext](https://huggingface.co/datasets/wikitext) dataset for demonstration purposes, but you can load your own dataset using one the other load components available on the [**Fondant Hub**](https://fondant.ai/en/latest/components/hub/#component-hub) or by creating your own [**custom load component**](https://fondant.ai/en/latest/guides/implement_custom_components/). Keep in mind that changing the dataset implies that you also need to change the [evaluation dataset](evaluation_datasets/wikitext_1000_q.csv) used in the evaluation pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose parameters over which to search**\n",
    "\n",
    "- `chunk_size`: size of each text chunk, in number of characters ([chunk text](https://github.com/ml6team/fondant/tree/main/components/chunk_text) component)\n",
    "- `chunk_overlap`: overlap between chunks ([chunk text](https://github.com/ml6team/fondant/tree/main/components/chunk_text) component)\n",
    "- `embed_model`: model used to embed ([embed text](https://github.com/ml6team/fondant/tree/main/components/embed_text) component)\n",
    "- `retrieval_top_k`: number of retrieved chunks taken into account for evaluation ([retrieve](https://github.com/ml6team/fondant/tree/main/components/retrieve_from_weaviate) component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose a search method**\n",
    "- [grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization): tries all possible combinations\n",
    "- progressive search (recommended): identifies best option per step, much more efficient as complexity increases linearly with number of search options vs. exponentially for grid search (with similar performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchable_index_params = {\n",
    "    'chunk_size' : [192, 256, 320],\n",
    "    'chunk_overlap' : [64, 128, 192],\n",
    "}\n",
    "searchable_shared_params = {\n",
    "    'embed_model' : [(\"huggingface\",\"all-MiniLM-L6-v2\"), (\"huggingface\", \"BAAI/bge-base-en-v1.5\")]\n",
    "}\n",
    "searchable_eval_params = {\n",
    "    'retrieval_top_k' : [2, 4, 8]\n",
    "}\n",
    "\n",
    "search_method = 'progressive_search' # 'grid_search', 'progressive_search'\n",
    "target_metric = 'context_precision' # relevant for 'smart' methods that use previous results to determine params, e.g. progressive search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è If you want to use **ChatGPT for evaluation** you will need an [OpenAI API key](https://platform.openai.com/docs/quickstart) (see TODO below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurable parameters\n",
    "shared_args = {\n",
    "    \"base_path\":\"./data\", # where data goes\n",
    "}\n",
    "index_args = {\n",
    "    \"n_rows_to_load\":1000,\n",
    "}\n",
    "eval_args = {\n",
    "    \"evaluation_set_path\" : \"./evaluation_datasets\",\n",
    "    \"evaluation_set_filename\" : \"wikitext_1000_q.csv\",\n",
    "    \"evaluation_set_separator\" : \";\",\n",
    "    \"evaluation_module\" : \"langchain.llms\",\n",
    "    \"evaluation_llm\" :\"OpenAI\",\n",
    "    \"evaluation_llm_kwargs\" : {\n",
    "                              \"openai_api_key\": os.environ[\"OPENAI_KEY\"], #TODO Specify your key if you're using OpenAI\n",
    "                              # \"model_name\" : \"gpt-3.5-turbo\"\n",
    "    },\n",
    "    \"evaluation_metrics\" : [\"context_precision\", \"context_relevancy\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° The first time you run a pipeline, you need to **download a docker image for each component** which may take a minute.\n",
    "\n",
    "> üí° Use a **GPU** or an external API to speed up the embedding step\n",
    "\n",
    "> üí° Steps that have been processed before are **cached** and will be skipped in subsequent runs which speeds up processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ParameterSearch\n",
    "\n",
    "mysearch = ParameterSearch(\n",
    "    searchable_index_params = searchable_index_params,\n",
    "    searchable_shared_params = searchable_shared_params,\n",
    "    searchable_eval_params = searchable_eval_params,\n",
    "    shared_args = shared_args,\n",
    "    index_args = index_args,\n",
    "    eval_args = eval_args,\n",
    "    search_method = search_method,\n",
    "    target_metric = target_metric\n",
    ")\n",
    "\n",
    "parameter_search_results = mysearch.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the performance** of your runs below. The default evaluation component uses [Ragas](https://github.com/explodinggradients/ragas) and provides the following two performance measures [context precision](https://docs.ragas.io/en/latest/concepts/metrics/context_precision.html) and [context relevancy](https://docs.ragas.io/en/latest/concepts/metrics/context_relevancy.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parameter_search_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mparameter_search_results\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parameter_search_results' is not defined"
     ]
    }
   ],
   "source": [
    "parameter_search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure **Plotly** is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"plotly\" --disable-pip-version-check && echo \"Plotly installed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show legend of **embedding models** used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import add_embed_model_numerical_column, show_legend_embed_models\n",
    "\n",
    "parameter_search_results = add_embed_model_numerical_column(parameter_search_results)\n",
    "show_legend_embed_models(parameter_search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "dimensions = ['chunk_size', 'chunk_overlap', 'embed_model_numerical', 'retrieval_top_k', 'context_precision']\n",
    "fig = px.parallel_coordinates(parameter_search_results, color=\"context_precision\",\n",
    "                              dimensions=dimensions,\n",
    "                              color_continuous_scale=px.colors.sequential.Bluered)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check your data and results at each step in the pipelines using the **Fondant data explorer**. The first time you run the data explorer, you need to download the docker image which may take a minute. Then you can access the data explorer at: **http://localhost:8501/**\n",
    "\n",
    "Enjoy the exploration! üç´ \n",
    "\n",
    "Press the ‚óºÔ∏è in the notebook toolbar to **stop the explorer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using local base path: ./data\n",
      "INFO:root:This directory will be mounted to /artifacts in the container.\n",
      "INFO:root:Running image from registry: fndnt/data_explorer with tag: 0.8.0 on port: 8501\n",
      "INFO:root:Access the explorer at http://localhost:8501\n"
     ]
    }
   ],
   "source": [
    "from fondant.explore import run_explorer_app\n",
    "\n",
    "run_explorer_app(base_path=shared_args[\"base_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up your environment\n",
    "\n",
    "After your pipeline run successfully, you can **clean up** your environment and stop the weaviate database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f weaviate/docker-compose.yaml down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please share your experience or **let us know how we can improve** through our \n",
    "* [**Discord**](https://discord.gg/HnTdWhydGp) \n",
    "* [**GitHub**](https://github.com/ml6team/fondant)\n",
    "\n",
    "And of course feel free to give us a [**star** ‚≠ê](https://github.com/ml6team/fondant) if you like what we are doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
