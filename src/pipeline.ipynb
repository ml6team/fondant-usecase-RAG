{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🍫 Building a RAG indexing pipeline with Fondant\n",
    "\n",
    "> ⚠️ Please note that this notebook **is not** compatible with **Google Colab**. To complete the tutorial, you must \n",
    "> initiate Docker containers. Starting Docker containers within Google Colab is not supported.\n",
    "\n",
    "This repository demonstrates a Fondant data pipeline that ingests text\n",
    "data into a vector database. The pipeline uses four reusable Fondant components.  \n",
    "Additionally, we provide a Docker Compose setup for Weaviate, enabling local testing and\n",
    "development.\n",
    "\n",
    "### Pipeline overview\n",
    "\n",
    "The primary goal of this sample is to showcase how you can use a Fondant pipeline and reusable\n",
    "components to load, chunk and embed text, as well as ingest the text embeddings to a vector\n",
    "database.\n",
    "Pipeline Steps:\n",
    "\n",
    "- [Data Loading](https://github.com/ml6team/fondant/tree/main/components/load_from_parquet): The\n",
    "  pipeline begins by loading text data from a Parquet file, which serves as the\n",
    "  source for subsequent processing. For the minimal example we are using a dataset from Huggingface.\n",
    "- [Text Chunking](https://github.com/ml6team/fondant/tree/main/components/chunk_text): Text data is\n",
    "  chunked into manageable sections to prepare it for embedding. This\n",
    "  step\n",
    "  is crucial for performant RAG systems.\n",
    "- [Text Embedding](https://github.com/ml6team/fondant/tree/main/components/embed_text): We are using\n",
    "  a small HuggingFace model for the generation of text embeddings.\n",
    "  The `embed_text` component easily allows the usage of different models as well.\n",
    "- [Write to Weaviate](https://github.com/ml6team/fondant/tree/main/components/index_weaviate): The\n",
    "  final step of the pipeline involves writing the embedded text data to\n",
    "  a Weaviate database.\n",
    "\n",
    "## Environment\n",
    "### This section checks the prerequisites of your environment. Read any errors or warnings carefully.\n",
    "\n",
    "**Ensure a Python between version 3.8 and 3.10 is available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info < (3, 8, 0) or sys.version_info >= (3, 11, 0):\n",
    "    raise Exception(f\"A Python version between 3.8 and 3.10 is required. You are running {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if docker compose is installed and the docker daemon is running**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose version >/dev/null\n",
    "!docker info >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Fondant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the pipeline\n",
    "\n",
    "First of all, we need to initialize the pipeline, which includes specifying a name for your pipeline, providing a description, and setting a base_path. The base_path is used to store the pipeline artifacts and data generated by the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.pipeline import ComponentOp, Pipeline\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_PATH = \"./data-dir\"\n",
    "\n",
    "Path(BASE_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    pipeline_name=\"ingestion-pipeline\",  # Add a unique pipeline name to easily track your progress and data\n",
    "    pipeline_description=\"Pipeline to prepare and process data for building a RAG solution\",\n",
    "    base_path=BASE_PATH, # The demo pipelines uses a local directory to store the data.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, we will utilize a dataset available on Hugging Face. As such, we will use a reusable Fondant component `load_from_hf_hub`. The `load_from_hf_hub`` component is a generic one, which implies that we still need to customize the component specification file. We have to modify the dataframe schema defined in the produce section of the component.\n",
    "\n",
    "To achieve this, we can create a `fondant_component.yaml` file in the directory `components/load_from_hf_hub` with the following content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile components/load_from_hf_hub/fondant_component.yaml\n",
    "name: Load from huggingface hub\n",
    "description: Component that loads a dataset from huggingface hub\n",
    "image: fndnt/load_from_hf_hub:0.8.dev2\n",
    "\n",
    "produces:\n",
    "  text:\n",
    "    type: string\n",
    "\n",
    "args:\n",
    "  dataset_name:\n",
    "    description: Name of dataset on the hub\n",
    "    type: str\n",
    "  column_name_mapping:\n",
    "    description: Mapping of the consumed hub dataset to fondant column names\n",
    "    type: dict\n",
    "    default: {}\n",
    "  image_column_names:\n",
    "    description: Optional argument, a list containing the original image column names in case the \n",
    "      dataset on the hub contains them. Used to format the image from HF hub format to a byte string.\n",
    "    type: list\n",
    "    default: []\n",
    "  n_rows_to_load:\n",
    "    description: Optional argument that defines the number of rows to load. Useful for testing pipeline runs on a small scale\n",
    "    type: int\n",
    "    default: None\n",
    "  index_column:\n",
    "    description: Column to set index to in the load component, if not specified a default globally unique index will be set\n",
    "    type: str\n",
    "    default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Afterwards, we can initialize the component and add it to our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_hf_hub = ComponentOp(\n",
    "    component_dir=\"components/load_from_hf_hub\",\n",
    "    arguments={\n",
    "        # Add arguments\n",
    "        \"dataset_name\": \"wikitext@~parquet\",\n",
    "        \"n_rows_to_load\": 100\n",
    "    }\n",
    ")\n",
    "\n",
    "pipeline.add_op(load_from_hf_hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our pipeline consists of a single component that loads the dataset from HuggingFace Hub. We can proceed to add the other components. All of them are reusable components, and we can initialize them using the `ComponentOp.from_registry(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_text_op = ComponentOp.from_registry(\n",
    "    name=\"chunk_text\",\n",
    "    arguments={\n",
    "        \"chunk_size\": 512,\n",
    "        \"chunk_overlap\": 32,\n",
    "    }\n",
    ")\n",
    "\n",
    "embed_text_op = ComponentOp.from_registry(\n",
    "    name=\"embed_text\",\n",
    "    arguments={\n",
    "        \"model_provider\": \"huggingface\",\n",
    "        \"model\": \"all-MiniLM-L6-v2\",\n",
    "    }\n",
    ")\n",
    "\n",
    "index_weaviate_op = ComponentOp.from_registry(\n",
    "    name=\"index_weaviate\",\n",
    "    arguments={\n",
    "        \"weaviate_url\": \"http://host.docker.internal:8080\",\n",
    "        \"class_name\": \"index\",  # Add a unique class name to show up on the leaderboard\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the components in our pipeline. It is important to note that we will define dependencies between the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_op(chunk_text_op, dependencies=load_from_hf_hub)\n",
    "pipeline.add_op(embed_text_op, dependencies=chunk_text_op)\n",
    "pipeline.add_op(index_weaviate_op, dependencies=embed_text_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the pipeline\n",
    "\n",
    "The pipeline will load and process text data, then ingest the processed data into a vector database. Before executing the pipeline, we need to start the Weaviate database. Otherwise the pipeline execution will fail.\n",
    "\n",
    "To do this, we can utilize the Docker setup provided in the `weaviate` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using a MacBook with a M1 processor you have to make sure to set the docker default platform to linux/amd64\n",
    "import os\n",
    "os.environ[\"DOCKER_DEFAULT_PLATFORM\"]=\"linux/amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f weaviate/docker-compose.yaml up --detach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can execute our pipeline. \n",
    "Fondant provides multiple runners to run our pipeline:\n",
    "\n",
    "- A Docker runner for local execution\n",
    "- A Vertex AI runner for managed execution on Google Cloud\n",
    "- A Kubeflow Pipelines runner for execution anywhere\n",
    "Here we will use the DockerRunner for local execution, which utilizes docker-compose under the hood.\n",
    "\n",
    "The runner will download the reusable components from the component hub. Afterwards, you will see the components execute one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.pipeline.compiler import DockerCompiler\n",
    "from fondant.pipeline.runner import DockerRunner\n",
    "\n",
    "DockerCompiler().compile(pipeline, output_path=\"docker-compose.yaml\")\n",
    "DockerRunner().run(\"docker-compose.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset\n",
    "\n",
    "You can also explore the dataset using the fondant explorer, this enables you to visualize your output dataset at each component step. Use the side panel on the left to browse through the steps and subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.explore import run_explorer_app\n",
    "\n",
    "run_explorer_app(\n",
    "    base_path=BASE_PATH,\n",
    "    container=\"fndnt/data_explorer\",\n",
    "    port=8501\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop the Explorer and continue the notebook, press the stop button at the top of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your own component\n",
    "\n",
    "Certainly, you can create your own custom components and use them in the pipeline. Let's consider building a component that cleans our text articles. For demo purpose we will implement a component thats removes all empty lines.\n",
    "\n",
    "To implement a custom component, a couple of files need to be defined:\n",
    "\n",
    "- Fondant component specification\n",
    "- main.py script in a src folder\n",
    "- Dockerfile\n",
    "- requirements.txt\n",
    "\n",
    "If you want to learn more about the creating custom components checkout [our documentation](https://fondant.ai/en/latest/components/custom_component/).\n",
    "\n",
    "\n",
    "### Component specification\n",
    "\n",
    "The component specification is represented by a single `fondant_component.yaml` file. There you can define which fields your component consumes and produces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile components/text_cleaning/fondant_component.yaml\n",
    "name: Text cleaning component\n",
    "description: Clean text passages\n",
    "image: ghcr.io/ml6team/text_cleaning:dev\n",
    "\n",
    "consumes:\n",
    "  text_data:\n",
    "    type: string\n",
    "\n",
    "produces:\n",
    "  text_data:\n",
    "    type: string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main.py script\n",
    "\n",
    "The core logic of the component should be implemented in a `main.py` script in a folder called `src`. We can implement the text cleaning logic as a class. We will inherit from the base class `PandasTransformComponent`. The `PandasTransformComponent` operates on pandas dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile components/text_cleaning/src/main.py\n",
    "import logging\n",
    "import pandas as pd\n",
    "from fondant.component import PandasTransformComponent\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class TextCleaningComponent(PandasTransformComponent):\n",
    "    def __init__(self, *_):\n",
    "        \"\"\"Initialize your component\"\"\"\n",
    "\n",
    "    def remove_empty_lines(self, text):\n",
    "        lines = text.split(\"\\n\")\n",
    "        non_empty_lines = [line.strip() for line in lines if line.strip()]\n",
    "        return \"\\n\".join(non_empty_lines)\n",
    "\n",
    "    def transform(self, dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "        dataframe[(\"text\", \"data\")] = dataframe[(\"text\", \"data\")].apply(\n",
    "            lambda x: self.remove_empty_lines\n",
    "        )\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dockerfile \n",
    "The Dockerfile defines how to build the component into a Docker image. You can use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile components/text_cleaning/Dockerfile\n",
    "FROM --platform=linux/amd64 python:3.8-slim as base\n",
    "\n",
    "# System dependencies\n",
    "RUN apt-get update && \\\n",
    "    apt-get upgrade -y && \\\n",
    "    apt-get install git -y\n",
    "\n",
    "# Install requirements\n",
    "COPY requirements.txt /\n",
    "RUN pip3 install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Set the working directory to the component folder\n",
    "WORKDIR /component/src\n",
    "\n",
    "# Copy over src-files\n",
    "COPY src/ .\n",
    "\n",
    "ENTRYPOINT [\"fondant\", \"execute\", \"main\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements.txt\n",
    "\n",
    "In the requirements.txt we define all dependencies of the component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile components/text_cleaning/requirements.txt\n",
    "fondant[component]==0.8.dev2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the new component to the pipeline\n",
    "\n",
    "Now we can add the new component to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaning_op = ComponentOp(\n",
    "    component_dir=\"components/text_cleaning\",\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    pipeline_name=\"ingestion-pipeline\",  # Add a unique pipeline name to easily track your progress and data\n",
    "    pipeline_description=\"Pipeline to prepare and process data for building a RAG solution\",\n",
    "    base_path=BASE_PATH, # The demo pipelines uses a local directory to store the data.\n",
    ")\n",
    "\n",
    "# reconstruct the pipeline\n",
    "pipeline.add_op(load_from_hf_hub)\n",
    "pipeline.add_op(text_cleaning_op, dependencies=load_from_hf_hub)\n",
    "pipeline.add_op(chunk_text_op, dependencies=text_cleaning_op)\n",
    "pipeline.add_op(embed_text_op, dependencies=chunk_text_op)\n",
    "pipeline.add_op(index_weaviate_op, dependencies=embed_text_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you now recompile your pipeline, the new changes will be picked up and Fondant will automatically re-build the component with the changes included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DockerCompiler().compile(pipeline=pipeline, output_path=\"docker-compose.yml\")\n",
    "DockerRunner().run(\"docker-compose.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you restart the Explorer, you'll see that you can now select a second pipeline in the left panel and inspect your new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_explorer_app(\n",
    "    base_path=BASE_PATH,\n",
    "    container=\"fndnt/data_explorer\",\n",
    "    port=8501,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up your environment\n",
    "\n",
    "After your pipeline run successfully, you should clean up your environment and stop the weaviate database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f weaviate/docker-compose.yaml down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling up\n",
    "If you're happy with your dataset, it's time to scale up. Check [our documentation](https://fondant.ai/en/latest/pipeline/#compiling-and-running-a-pipeline) for more information about the available runners.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
