{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🍫 Building and Evaluate a RAG pipeline with Fondant\n",
    "\n",
    "> ⚠️ Please note that this notebook **is not** compatible with **Google Colab**. To complete the tutorial, you must \n",
    "> initiate Docker containers. Starting Docker containers within Google Colab is not supported.\n",
    "\n",
    "This repository demonstrates a complete RAG pipeline. \n",
    "\n",
    "The RAG pipeline is composed of a Fondant pipeline that ingests text data into a vector database (indexing pipeline), and a Fondant pipeline that proceeds retrieval on the vector database and evaluates it (evaluation pipeline). The score is then displayed to observe the performance of the chosen RAG configuration.\n",
    "\n",
    "Additionally, we provide a Docker Compose setup for Weaviate, enabling local testing and\n",
    "development, and an evaluation dataset containing questions over the first 1000 rows of the [loaded text data](https://huggingface.co/datasets/wikitext)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ Pay attention to the \"TODO\"s in the notebook as they require your input to run the pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Fondant for RAG?\n",
    "\n",
    "You will quickly notice that this notebook may look like another basic RAG pipeline notebook. So what is the difference here? Fondant!\n",
    "\n",
    "Leveraging Fondant is key to reach the best RAG configuration for your use case in a record time. Finding the best RAG configuration implies several runs with different parameters (the size of the chunks, the embedding model used, the retrieval strategy, some data processing...). However, a single run can already quickly take a while, imagine for 10 or 50 runs. \n",
    "\n",
    "Fondant enables you to:\n",
    "- Easily perform several runs, where distributed processing and caching will be automatically managed to run the pipelines as efficiently as possible.\n",
    "- Explore each pipeline configuration\n",
    "- Compare them with different metrics\n",
    "- Finally, choose your best RAG configuration. Just like a hyper-parametrisation!\n",
    "\n",
    "This Notebook is a first sight on Fondant's capabilities in a RAG context, but can be definitely improved as much as RAG keeps evoluating with new techniques! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 💡 If you want to do a Grid-Search over different RAG configurations, take a look at our `grid_search` notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if docker compose is installed and the docker daemon is running**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation\n",
    "!docker compose version >/dev/null\n",
    "!docker info >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Fondant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initiate the weaviate vectorDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using a MacBook with a M1 processor you have to make sure to set the docker default platform to linux/amd64\n",
    "import os\n",
    "os.environ[\"DOCKER_DEFAULT_PLATFORM\"]=\"linux/amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Weaviate with Docker compose\n",
    "!docker compose -f weaviate/docker-compose.yaml up --detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the vectorDB is running and accessible\n",
    "import weaviate\n",
    "local_weaviate_client = weaviate.Client(\"http://localhost:8080\")\n",
    "local_weaviate_client.schema.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the RAG Pipeline and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline overview\n",
    "\n",
    "The RAG pipeline is divided into two pipelines:\n",
    "\n",
    "- **Indexing Pipeline**: This pipeline processes text data and loads it in an indexed vector database.\n",
    "- **Evaluation Pipeline**: This pipeline proceeds retrieval from the vector database (based on a set of queries) and evaluates the results using [RAGAS](https://docs.ragas.io/en/latest/index.html), a RAG evaluation framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the pipelines creator and the pipeline runner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.pipeline.runner import DockerRunner\n",
    "import pipeline_index, pipeline_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up RAG parameters and Run the Pipelines\n",
    "\n",
    "**Set-up shared parameters**\n",
    "\n",
    "Both Indexing and Evaluation pipeline must share parameters that are set-up below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Host IP address to be able to access the vector database from a Docker Image\n",
    "import socket\n",
    "\n",
    "def get_host_ip():\n",
    "    try:\n",
    "        # Create a socket object and connect to an external server\n",
    "        # This step is done to get the local machine's IP address\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        s.connect((\"8.8.8.8\", 80))\n",
    "        host_ip = s.getsockname()[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error while retrieving host IP address: {e}\")\n",
    "        host_ip = None\n",
    "    finally:\n",
    "        s.close()\n",
    "\n",
    "    return host_ip\n",
    "\n",
    "# Example usage\n",
    "host_ip = get_host_ip()\n",
    "print(f\"Host IP address: {host_ip}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_args = {\n",
    "    \"pipeline_dir\":\"./data-dir\",\n",
    "    \"embed_model_provider\":\"huggingface\",\n",
    "    \"embed_model\":\"all-MiniLM-L6-v2\",\n",
    "    \"weaviate_url\":f\"http://{host_ip}:8080\", # IP address \n",
    "    \"weaviate_class_name\":\"Pipeline1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set-up and Run Indexing Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Indexing pipeline created can be found in `pipeline_index.py`. It is composed of 4 steps/components:\n",
    "\n",
    "- [Data Loading](https://github.com/ml6team/fondant/tree/main/components/load_from_parquet): The\n",
    "  pipeline begins by loading text data from a Parquet file, which serves as the\n",
    "  source for subsequent processing. For the minimal example we are using a dataset from Huggingface.\n",
    "- [Text Chunking](https://github.com/ml6team/fondant/tree/main/components/chunk_text): Text data is\n",
    "  chunked into manageable sections to prepare it for embedding. This\n",
    "  step\n",
    "  is crucial for performant RAG systems.\n",
    "- [Text Embedding](https://github.com/ml6team/fondant/tree/main/components/embed_text): We are using\n",
    "  a small HuggingFace model for the generation of text embeddings.\n",
    "  The `embed_text` component easily allows the usage of different models as well. It can be modified in the `fixed_args` dictionary above.\n",
    "- [Write to Weaviate](https://github.com/ml6team/fondant/tree/main/components/index_weaviate): The\n",
    "  final step of the pipeline involves writing the embedded text data to\n",
    "  a Weaviate database.\n",
    "\n",
    "Below are specified the arguments of the indexing pipeline which can be modified. Keep in mind that changing the dataset implies changing the evaluation dataset used in the Evaluation Pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_args = {\n",
    "    \"hf_dataset_name\":\"wikitext@~parquet\",\n",
    "    \"data_column_name\":\"text\",\n",
    "    \"n_rows_to_load\":1000,\n",
    "    \"chunk_size\":512,\n",
    "    \"chunk_overlap\":32\n",
    "}\n",
    "\n",
    "indexing_pipeline = pipeline_index.create_pipeline(**fixed_args, **indexing_args)\n",
    "\n",
    "# indexing_pipeline = pipeline_index.create_pipeline(\n",
    "#     pipeline_dir=\"./data-dir\",\n",
    "#     embed_model_provider=\"huggingface\",\n",
    "#     embed_model=\"all-MiniLM-L6-v2\",\n",
    "#     weaviate_url=f\"{host_ip}:8080\", # IP address \n",
    "#     weaviate_class_name=\"Pipeline_1\",\n",
    "#     hf_dataset_name=\"wikitext@~parquet\",\n",
    "#     data_column_name=\"text\",\n",
    "#     n_rows_to_load=1000,\n",
    "#     chunk_size=512,\n",
    "#     chunk_overlap=32\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_indexing_pipeline(runner, index_pipeline, host_ip, weaviate_class_name):\n",
    "    runner.run(index_pipeline)\n",
    "    docker_weaviate_client = weaviate.Client(f\"http://{host_ip}:8080\")\n",
    "    return docker_weaviate_client.schema.get(weaviate_class_name)\n",
    "\n",
    "runner = DockerRunner()\n",
    "weaviate_class_name = fixed_args[\"weaviate_class_name\"]\n",
    "\n",
    "run_indexing_pipeline(\n",
    "    runner=runner,\n",
    "    index_pipeline=indexing_pipeline,\n",
    "    host_ip=host_ip,\n",
    "    weaviate_class_name=weaviate_class_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set-up and Run Evaluation Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Evaluation Pipeline created can be found in `pipeline_eval.py`. It is composed of 5 steps/components:\n",
    "\n",
    "- **Evaluation Data Loading**: The pipeline begins by loading the evaluation dataset from a CSV file. The dataset contains a set of questions over the HuggingFace loaded dataset that will be used for the retrieval and the evaluation. \n",
    "- **Text Embedding**: The questions' embeddings are generated using the same model as for the HuggingFace loaded dataset.\n",
    "- **Retrieve Relevant Documents**: For each question's embedding, the revelant documents to answer it are retrieved from the vector database. \n",
    "- **Evaluate the RAG configuration**: This component evaluates the RAG pipeline using metrics computed using [RAGAS](https://docs.ragas.io/en/latest/index.html)' evaluation framework. The metrics computed are the [precision](https://docs.ragas.io/en/latest/concepts/metrics/context_precision.html) and the [relevancy](https://docs.ragas.io/en/latest/concepts/metrics/context_relevancy.html) of the retrieved context.\n",
    "- **Aggregate Evaluation Scores**: This last component aggregates the scores over all questions for each metric computed.\n",
    "\n",
    "Below are specified the arguments of the evaluation pipeline which can be modified. Keep in mind that if the dataset loaded in the indexing pipeline was changed, the evaluation dataset must be as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_args = {\n",
    "    \"csv_dataset_uri\":\"/data/wikitext_1000_q.csv\",\n",
    "    \"csv_column_separator\":\";\",\n",
    "    \"question_column_name\":\"question\",\n",
    "    \"top_k\":3,\n",
    "    \"module\": \"langchain.llms\",\n",
    "    \"llm_name\":\"OpenAI\",\n",
    "    \"llm_kwargs\":{\"openai_api_key\": \"\"}, #TODO Specify your key in you're using OpenAI\n",
    "    \"metrics\":[\"context_precision\", \"context_relevancy\"]\n",
    "}\n",
    "\n",
    "evaluation_pipeline = pipeline_eval.create_pipeline(**fixed_args, **evaluation_args)\n",
    "\n",
    "# evaluation_pipeline = pipeline_eval.create_pipeline(\n",
    "#     pipeline_dir=\"./data-dir\",\n",
    "#     embed_model_provider=\"huggingface\",\n",
    "#     embed_model=\"all-MiniLM-L6-v2\",\n",
    "#     weaviate_url=f\"{host_ip}:8080\", # IP address \n",
    "#     weaviate_class_name=\"Pipeline_1\",\n",
    "#     csv_dataset_uri=\"/data/wikitext_1000_q.csv\", #make sure it is the same as mounted file\n",
    "#     csv_column_separator=\";\",\n",
    "#     question_column_name=\"question\",\n",
    "#     top_k=3,\n",
    "#     llm_name=\"OpenAI\",\n",
    "#     llm_kwargs={\"openai_api_key\": \"\"},\n",
    "#     metrics=[\"context_precision\", \"context_relevancy\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation_pipeline(runner, eval_pipeline, extra_volumes):\n",
    "    runner.run(input=eval_pipeline, extra_volumes=extra_volumes)\n",
    "\n",
    "runner = DockerRunner()\n",
    "local_folder_absolute_path = \"fondant-usecase-RAG/src/local_file\" #TODO Repace with absolute Path\n",
    "extra_volumes = [f\"{local_folder_absolute_path}:/data\"]\n",
    "\n",
    "run_evaluation_pipeline(\n",
    "    runner=runner,\n",
    "    eval_pipeline=evaluation_pipeline,\n",
    "    extra_volumes=extra_volumes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset\n",
    "\n",
    "You can explore your results using the fondant explorer, this enables you to visualize your output dataset at each component step. It might take a while to start the first time as it needs to download the explorer docker image first. \n",
    "\n",
    "Enjoy the exploration! 🍫 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.explore import run_explorer_app\n",
    "\n",
    "run_explorer_app(base_path=fixed_args[\"pipeline_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Latest Evaluated Pipeline Score**\n",
    "\n",
    "You can also read the latest dataset containing the results of the scoring of your RAG pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read latest chosen component\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_latest_data(base_path: str, pipeline_name: str, component_name: str):\n",
    "    # Specify the path to the 'data' directory\n",
    "    data_directory = f\"{base_path}/{pipeline_name}\"\n",
    "\n",
    "    # Get a list of all subdirectories in the 'data' directory\n",
    "    subdirectories = [\n",
    "        d\n",
    "        for d in os.listdir(data_directory)\n",
    "        if os.path.isdir(os.path.join(data_directory, d))\n",
    "    ]\n",
    "\n",
    "    # keep pipeline directories\n",
    "    valid_entries = [\n",
    "        entry for entry in subdirectories if entry.startswith(pipeline_name)\n",
    "    ]\n",
    "    # keep pipeline folders containing a parquet file in the component folder\n",
    "    valid_entries = [\n",
    "        folder\n",
    "        for folder in valid_entries\n",
    "        if has_parquet_file(data_directory, folder, component_name)\n",
    "    ]\n",
    "    # keep the latest folder\n",
    "    latest_folder = sorted(valid_entries, key=extract_timestamp, reverse=True)[0]\n",
    "\n",
    "    # If a valid folder is found, proceed to read all Parquet files in the component folder\n",
    "    if latest_folder:\n",
    "        # Find the path to the component folder\n",
    "        component_folder = os.path.join(data_directory, latest_folder, component_name)\n",
    "\n",
    "        # Get a list of all Parquet files in the component folder\n",
    "        parquet_files = [\n",
    "            f for f in os.listdir(component_folder) if f.endswith(\".parquet\")\n",
    "        ]\n",
    "\n",
    "        if parquet_files:\n",
    "            # Read all Parquet files and concatenate them into a single DataFrame\n",
    "            dfs = [\n",
    "                pd.read_parquet(os.path.join(component_folder, file))\n",
    "                for file in parquet_files\n",
    "            ]\n",
    "            return pd.concat(dfs, ignore_index=True)\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def has_parquet_file(data_directory, entry, component_name):\n",
    "    component_folder = os.path.join(data_directory, entry, component_name)\n",
    "    # Check if the component exists\n",
    "    if not os.path.exists(component_folder) or not os.path.isdir(component_folder):\n",
    "        return False\n",
    "    parquet_files = [\n",
    "        file for file in os.listdir(component_folder) if file.endswith(\".parquet\")\n",
    "    ]\n",
    "    return bool(parquet_files)\n",
    "\n",
    "\n",
    "def extract_timestamp(folder_name):\n",
    "    # Extract the timestamp part from the folder name\n",
    "    timestamp_str = folder_name.split(\"-\")[-1]\n",
    "    # Convert the timestamp string to a datetime object\n",
    "    return datetime.strptime(timestamp_str, \"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read aggregated results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dir = \"./data-dir\"\n",
    "pipeline_name = \"evaluation-pipeline\"\n",
    "component_name = \"aggregate_eval_results\"\n",
    "\n",
    "read_latest_data(\n",
    "            base_path=pipeline_dir,\n",
    "            pipeline_name=pipeline_name,\n",
    "            component_name=component_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up your environment\n",
    "\n",
    "After your pipeline run successfully, you should clean up your environment and stop the weaviate database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f weaviate/docker-compose.yaml down"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
