{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation\n",
    "!docker compose version >/dev/null\n",
    "!docker info >/dev/null\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using a MacBook with a M1 processor you have to make sure to set the docker default platform to linux/amd64\n",
    "import os\n",
    "os.environ[\"DOCKER_DEFAULT_PLATFORM\"]=\"linux/amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to have your weaviate vectorDB running\n",
    "!docker compose -f weaviate_local/docker-compose.yml up --detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "local_weaviate_client = weaviate.Client(\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from fondant.pipeline import ComponentOp, Pipeline\n",
    "from pathlib import Path\n",
    "\n",
    "from fondant.pipeline.runner import DockerRunner\n",
    "import pipeline_index, pipeline_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Host IP address\n",
    "import socket\n",
    "\n",
    "def get_host_ip():\n",
    "    try:\n",
    "        # Create a socket object and connect to an external server (e.g., Google's public DNS server)\n",
    "        # This step is done to get the local machine's IP address\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        s.connect((\"8.8.8.8\", 80))\n",
    "        host_ip = s.getsockname()[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error while retrieving host IP address: {e}\")\n",
    "        host_ip = None\n",
    "    finally:\n",
    "        s.close()\n",
    "\n",
    "    return host_ip\n",
    "\n",
    "# Example usage\n",
    "host_ip = get_host_ip()\n",
    "print(f\"Host IP address: {host_ip}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_args = {\n",
    "    \"pipeline_dir\":\"./data-dir\",\n",
    "    \"embed_model_provider\":\"huggingface\",\n",
    "    \"embed_model\":\"all-MiniLM-L6-v2\",\n",
    "    \"weaviate_url\":f\"http://{host_ip}:8080\", # IP address \n",
    "    \"weaviate_class_name\":\"Pipeline_1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up the indexing pipeline\n",
    "indexing_args = {\n",
    "    \"hf_dataset_name\":\"wikitext@~parquet\",\n",
    "    \"data_column_name\":\"text\",\n",
    "    \"n_rows_to_load\":1000,\n",
    "    \"chunk_size\":512,\n",
    "    \"chunk_overlap\":32\n",
    "}\n",
    "\n",
    "indexing_pipeline = pipeline_index.create_pipeline(**fixed_args, **indexing_args)\n",
    "\n",
    "# indexing_pipeline = pipeline_index.create_pipeline(\n",
    "#     pipeline_dir=\"./data-dir\",\n",
    "#     embed_model_provider=\"huggingface\",\n",
    "#     embed_model=\"all-MiniLM-L6-v2\",\n",
    "#     weaviate_url=f\"{host_ip}:8080\", # IP address \n",
    "#     weaviate_class_name=\"Pipeline_1\",\n",
    "#     hf_dataset_name=\"wikitext@~parquet\",\n",
    "#     data_column_name=\"text\",\n",
    "#     n_rows_to_load=1000,\n",
    "#     chunk_size=512,\n",
    "#     chunk_overlap=32\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the indexing pipeline\n",
    "def run_indexing_pipeline(runner, index_pipeline, host_ip, weaviate_class_name):\n",
    "    runner.run(index_pipeline)\n",
    "    docker_weaviate_client = weaviate.Client(f\"http://{host_ip}:8080\")\n",
    "    return docker_weaviate_client.schema.get(weaviate_class_name)\n",
    "\n",
    "runner = DockerRunner()\n",
    "weaviate_class_name = \"Pipeline_1\"\n",
    "\n",
    "run_indexing_pipeline(runner=runner, index_pipeline=indexing_pipeline, host_ip=host_ip, weaviate_class_name=weaviate_class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up the evaluation pipeline\n",
    "evaluation_args = {\n",
    "    \"csv_dataset_uri\":\"/data/wikitext_1000_q.csv\", #make sure it is the same as mounted file\n",
    "    \"csv_column_separator\":\";\",\n",
    "    \"question_column_name\":\"question\",\n",
    "    \"top_k\":3,\n",
    "    \"llm_name\":\"OpenAI\",\n",
    "    \"llm_kwargs\":{\"openai_api_key\": \"\"}, #TODO\n",
    "    \"metrics\":[\"context_precision\", \"context_relevancy\"]\n",
    "}\n",
    "\n",
    "evaluation_pipeline = pipeline_eval.create_pipeline(**fixed_args, **evaluation_args)\n",
    "\n",
    "# evaluation_pipeline = pipeline_eval.create_pipeline(\n",
    "#     pipeline_dir=\"./data-dir\",\n",
    "#     embed_model_provider=\"huggingface\",\n",
    "#     embed_model=\"all-MiniLM-L6-v2\",\n",
    "#     weaviate_url=f\"{host_ip}:8080\", # IP address \n",
    "#     weaviate_class_name=\"Pipeline_1\",\n",
    "#     csv_dataset_uri=\"/data/wikitext_1000_q.csv\", #make sure it is the same as mounted file\n",
    "#     csv_column_separator=\";\",\n",
    "#     question_column_name=\"question\",\n",
    "#     top_k=3,\n",
    "#     llm_name=\"OpenAI\",\n",
    "#     llm_kwargs={\"openai_api_key\": \"\"},\n",
    "#     metrics=[\"context_precision\", \"context_relevancy\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the evaluation pipeline\n",
    "def run_evaluation_pipeline(runner, eval_pipeline, extra_volumes):\n",
    "    runner.run(input=eval_pipeline, extra_volumes=extra_volumes)\n",
    "\n",
    "runner = DockerRunner()\n",
    "local_folder_absolute_path = \"/Users/hakimamri/Documents/GitHub/fondant-usecase-RAG/src/local_file\" #TODO\n",
    "extra_volumes = [f\"{local_folder_absolute_path}:/data\"]\n",
    "run_evaluation_pipeline(runner, evaluation_pipeline, extra_volumes=extra_volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read latest chosen component\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_latest_data(base_path: str, pipeline_name: str, component_name: str):\n",
    "    # Specify the path to the 'data' directory\n",
    "    data_directory = f\"{base_path}/{pipeline_name}\"\n",
    "\n",
    "    # Get a list of all subdirectories in the 'data' directory\n",
    "    subdirectories = [\n",
    "        d\n",
    "        for d in os.listdir(data_directory)\n",
    "        if os.path.isdir(os.path.join(data_directory, d))\n",
    "    ]\n",
    "\n",
    "    # keep pipeline directories\n",
    "    valid_entries = [\n",
    "        entry for entry in subdirectories if entry.startswith(pipeline_name)\n",
    "    ]\n",
    "    # keep pipeline folders containing a parquet file in the component folder\n",
    "    valid_entries = [\n",
    "        folder\n",
    "        for folder in valid_entries\n",
    "        if has_parquet_file(data_directory, folder, component_name)\n",
    "    ]\n",
    "    # keep the latest folder\n",
    "    latest_folder = sorted(valid_entries, key=extract_timestamp, reverse=True)[0]\n",
    "\n",
    "    # If a valid folder is found, proceed to read all Parquet files in the component folder\n",
    "    if latest_folder:\n",
    "        # Find the path to the component folder\n",
    "        component_folder = os.path.join(data_directory, latest_folder, component_name)\n",
    "\n",
    "        # Get a list of all Parquet files in the component folder\n",
    "        parquet_files = [\n",
    "            f for f in os.listdir(component_folder) if f.endswith(\".parquet\")\n",
    "        ]\n",
    "\n",
    "        if parquet_files:\n",
    "            # Read all Parquet files and concatenate them into a single DataFrame\n",
    "            dfs = [\n",
    "                pd.read_parquet(os.path.join(component_folder, file))\n",
    "                for file in parquet_files\n",
    "            ]\n",
    "            return pd.concat(dfs, ignore_index=True)\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def has_parquet_file(data_directory, entry, component_name):\n",
    "    component_folder = os.path.join(data_directory, entry, component_name)\n",
    "    # Check if the component exists\n",
    "    if not os.path.exists(component_folder) or not os.path.isdir(component_folder):\n",
    "        return False\n",
    "    parquet_files = [\n",
    "        file for file in os.listdir(component_folder) if file.endswith(\".parquet\")\n",
    "    ]\n",
    "    return bool(parquet_files)\n",
    "\n",
    "\n",
    "def extract_timestamp(folder_name):\n",
    "    # Extract the timestamp part from the folder name\n",
    "    timestamp_str = folder_name.split(\"-\")[-1]\n",
    "    # Convert the timestamp string to a datetime object\n",
    "    return datetime.strptime(timestamp_str, \"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dir = \"./data-dir\"\n",
    "read_latest_data(\n",
    "            base_path=pipeline_dir,\n",
    "            pipeline_name=\"evaluation-pipeline\",\n",
    "            component_name=\"aggregate_eval_results\",\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
