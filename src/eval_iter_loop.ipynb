{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using a MacBook with a M1 processor you have to make sure to set the docker default platform to linux/amd64\n",
    "import os\n",
    "os.environ[\"DOCKER_DEFAULT_PLATFORM\"]=\"linux/amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation\n",
    "!docker compose version >/dev/null\n",
    "!docker info >/dev/null\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from fondant.pipeline import ComponentOp, Pipeline\n",
    "from pathlib import Path\n",
    "\n",
    "from fondant.pipeline.compiler import DockerCompiler\n",
    "from fondant.pipeline.runner import DockerRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to run the weaviate instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "weaviate_client = weaviate.Client(\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterative_eval_abstracts.main import IterativeEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_eval = IterativeEvaluation(\n",
    "    pipeline_dir=\"./data-dir\",\n",
    "    embed_model_provider=\"huggingface\",\n",
    "    embed_model=\"all-MiniLM-L6-v2\",\n",
    "    weaviate_url=\"http://192.168.64.1:8080\",\n",
    "    weaviate_class_name=\"Pipeline_1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = iter_eval.run_indexing_pipeline(\n",
    "    hf_dataset_name=\"wikitext@~parquet\",\n",
    "    data_column_name=\"text\",\n",
    "    n_rows_to_load=1000,\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = iter_eval.run_evaluation_pipeline(\n",
    "    csv_dataset_uri=\"/data/wikitext_1000_q.csv\", #make sure it is the same as mounted file\n",
    "    csv_column_separator=\";\",\n",
    "    question_column_name=\"question\",\n",
    "    top_k=3,\n",
    "    llm_name=\"OpenAI\",\n",
    "    llm_kwargs={\"openai_api_key\": \"\"},\n",
    "    metrics=[\"context_precision\", \"context_relevancy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display aggregated results\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-Search (not yet developed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid-search/iterative loop\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Define the values for grid search\n",
    "chunk_sizes = [128, 256, 512]\n",
    "chunk_overlaps = [10, 25, 50]\n",
    "embed_models = [(\"huggingface\",\"all-MiniLM-L6-v2\"), (\"huggingface\"\"BAAI/bge-small-en\"), (\"huggingface\"\"BAAI/bge-large-zh-v1.5\")]\n",
    "top_k = [2, 3, 5]\n",
    "\n",
    "# Fixed parameters\n",
    "hf_dataset_name = \"wikitext@~parquet\"\n",
    "data_column_name = \"text\"\n",
    "load_sample = True\n",
    "weaviate_url = \"http://host.docker.internal:8080\"\n",
    "csv_dataset_uri = \"/data/wikitext_1000_q.csv\"\n",
    "csv_column_separator = \";\"\n",
    "question_column_name = \"question\"\n",
    "llm_name = \"OpenAI\"\n",
    "llm_kwargs = {\"openai_api_key\": ''} \n",
    "metrics = [\"context_precision\", \"context_relevancy\"]\n",
    "\n",
    "# Results dictionary to store results for each iteration\n",
    "results_dict = {}\n",
    "\n",
    "# Perform grid search\n",
    "for i, chunk_size, chunk_overlap, embed_model in enumerate(itertools.product(chunk_sizes, chunk_overlaps, embed_models, top_k), start=1):\n",
    "    # Call the run_evaluation_pipeline function with the current parameter combination\n",
    "    indexing_class_name = f\"Index_{i}\"\n",
    "    logging.info(f\"Running indexing for iteration {i} with chunk_size={chunk_size}, chunk_overlap={chunk_overlap}, embed_model={embed_model}\")\n",
    "    \n",
    "    run_indexing_pipeline(\n",
    "        hf_dataset_name=hf_dataset_name,\n",
    "        data_column_name=data_column_name,\n",
    "        load_sample=load_sample,\n",
    "        chunk_size=chunk_sizes,\n",
    "        chunk_overlap=chunk_overlaps,\n",
    "        embed_model_provider=embed_models[0],\n",
    "        embed_model=embed_models[1],\n",
    "        weaviate_url=weaviate_url,\n",
    "        weaviate_class_name=indexing_class_name\n",
    "    )\n",
    "\n",
    "    # Run evaluation pipeline\n",
    "    evaluation_class_name = f\"Index_{i}\"\n",
    "    logging.info(f\"Running evaluation for iteration {i} with top_k={top_k}\")\n",
    "\n",
    "    results = run_evaluation_pipeline(\n",
    "        csv_dataset_uri=csv_dataset_uri,\n",
    "        csv_column_separator=csv_column_separator,\n",
    "        question_column_name=question_column_name,\n",
    "        embed_model_provider=embed_models[0],\n",
    "        embed_model=embed_models[1],\n",
    "        weaviate_url=weaviate_url,\n",
    "        weaviate_class_name=indexing_class_name,\n",
    "        top_k=top_k,  # Set your desired top_k value\n",
    "        llm_name=llm_name,\n",
    "        llm_kwargs=llm_kwargs\n",
    "        metrics=metrics  # Set your desired metrics\n",
    "    )\n",
    "\n",
    "    # Save the results in the dictionary\n",
    "    results_dict[(chunk_size, chunk_overlap, embed_model, top_k)] = results #results should be a dictionary with {\"metric 1\": x, \"metric 2\": y}\n",
    "\n",
    "# Print or use the results as needed\n",
    "print(results_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
