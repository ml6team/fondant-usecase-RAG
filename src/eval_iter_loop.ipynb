{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation\n",
    "!docker compose version >/dev/null\n",
    "!docker info >/dev/null\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using a MacBook with a M1 processor you have to make sure to set the docker default platform to linux/amd64\n",
    "import os\n",
    "os.environ[\"DOCKER_DEFAULT_PLATFORM\"]=\"linux/amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from fondant.pipeline import ComponentOp, Pipeline\n",
    "from pathlib import Path\n",
    "\n",
    "from fondant.pipeline.runner import DockerRunner\n",
    "import pipeline_index, pipeline_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to run the weaviate instance\n",
    "import weaviate\n",
    "local_weaviate_client = weaviate.Client(\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_args = {\n",
    "    \"pipeline_dir\":\"./data-dir\",\n",
    "    \"embed_model_provider\":\"huggingface\",\n",
    "    \"embed_model\":\"all-MiniLM-L6-v2\",\n",
    "    \"weaviate_url\":\":8080\", # IP address \n",
    "    \"weaviate_class_name\":\"Pipeline_2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up the pipeline\n",
    "indexing_args = {\n",
    "    \"hf_dataset_name\":\"wikitext@~parquet\",\n",
    "    \"data_column_name\":\"text\",\n",
    "    \"n_rows_to_load\":1000,\n",
    "    \"chunk_size\":256,\n",
    "    \"chunk_overlap\":16\n",
    "}\n",
    "\n",
    "indexing_pipeline = pipeline_index.create_pipeline(**fixed_args, **indexing_args)\n",
    "\n",
    "# indexing_pipeline = pipeline_index.create_pipeline(\n",
    "#     pipeline_dir=\"./data-dir\",\n",
    "#     embed_model_provider=\"huggingface\",\n",
    "#     embed_model=\"all-MiniLM-L6-v2\",\n",
    "#     weaviate_url=\":8080\", # IP address \n",
    "#     weaviate_class_name=\"Pipeline_1\",\n",
    "#     hf_dataset_name=\"wikitext@~parquet\",\n",
    "#     data_column_name=\"text\",\n",
    "#     n_rows_to_load=1000,\n",
    "#     chunk_size=512,\n",
    "#     chunk_overlap=32\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the pipeline\n",
    "def run_indexing_pipeline(runner, index_pipeline, docker_weaviate_client, weaviate_class_name):\n",
    "    runner.run(index_pipeline)\n",
    "    return docker_weaviate_client.schema.get(weaviate_class_name)\n",
    "\n",
    "runner = DockerRunner()\n",
    "docker_weaviate_client = weaviate.Client(\"http://192.168.64.1:8080\")\n",
    "weaviate_class_name = \"Pipeline_1\"\n",
    "\n",
    "run_indexing_pipeline(runner=runner, index_pipeline=indexing_pipeline, docker_weaviate_client=docker_weaviate_client, weaviate_class_name=weaviate_class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up the pipeline\n",
    "evaluation_args = {\n",
    "    \"csv_dataset_uri\":\"/data/wikitext_1000_q.csv\", #make sure it is the same as mounted file\n",
    "    \"csv_column_separator\":\";\",\n",
    "    \"question_column_name\":\"question\",\n",
    "    \"top_k\":3,\n",
    "    \"llm_name\":\"OpenAI\",\n",
    "    \"llm_kwargs\":{\"openai_api_key\": \"\"},\n",
    "    \"metrics\":[\"context_precision\", \"context_relevancy\"]\n",
    "}\n",
    "\n",
    "evaluation_pipeline = pipeline_eval.create_pipeline(**fixed_args, **evaluation_args)\n",
    "\n",
    "# evaluation_pipeline = pipeline_eval.create_pipeline(\n",
    "#     pipeline_dir=\"./data-dir\",\n",
    "#     embed_model_provider=\"huggingface\",\n",
    "#     embed_model=\"all-MiniLM-L6-v2\",\n",
    "#     weaviate_url=\":8080\", # IP address \n",
    "#     weaviate_class_name=\"Pipeline_1\",\n",
    "#     csv_dataset_uri=\"/data/wikitext_1000_q.csv\", #make sure it is the same as mounted file\n",
    "#     csv_column_separator=\";\",\n",
    "#     question_column_name=\"question\",\n",
    "#     top_k=3,\n",
    "#     llm_name=\"OpenAI\",\n",
    "#     llm_kwargs={\"openai_api_key\": \"\"},\n",
    "#     metrics=[\"context_precision\", \"context_relevancy\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the pipeline\n",
    "def run_evaluation_pipeline(runner, eval_pipeline, extra_volumes):\n",
    "    runner.run(input=eval_pipeline, extra_volumes=extra_volumes)\n",
    "\n",
    "runner = DockerRunner()\n",
    "extra_volumes = [\"/Users/hakimamri/Documents/GitHub/fondant-usecase-RAG/src/local_file:/data\"]\n",
    "run_evaluation_pipeline(runner, evaluation_pipeline, extra_volumes=extra_volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read latest chosen component\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_latest_data(base_path: str, pipeline_name: str, component_name: str):\n",
    "    # Specify the path to the 'data' directory\n",
    "    data_directory = f\"{base_path}/{pipeline_name}\"\n",
    "\n",
    "    # Get a list of all subdirectories in the 'data' directory\n",
    "    subdirectories = [\n",
    "        d\n",
    "        for d in os.listdir(data_directory)\n",
    "        if os.path.isdir(os.path.join(data_directory, d))\n",
    "    ]\n",
    "\n",
    "    # keep pipeline directories\n",
    "    valid_entries = [\n",
    "        entry for entry in subdirectories if entry.startswith(pipeline_name)\n",
    "    ]\n",
    "    # keep pipeline folders containing a parquet file in the component folder\n",
    "    valid_entries = [\n",
    "        folder\n",
    "        for folder in valid_entries\n",
    "        if has_parquet_file(data_directory, folder, component_name)\n",
    "    ]\n",
    "    # keep the latest folder\n",
    "    latest_folder = sorted(valid_entries, key=extract_timestamp, reverse=True)[0]\n",
    "\n",
    "    # If a valid folder is found, proceed to read all Parquet files in the component folder\n",
    "    if latest_folder:\n",
    "        # Find the path to the component folder\n",
    "        component_folder = os.path.join(data_directory, latest_folder, component_name)\n",
    "\n",
    "        # Get a list of all Parquet files in the component folder\n",
    "        parquet_files = [\n",
    "            f for f in os.listdir(component_folder) if f.endswith(\".parquet\")\n",
    "        ]\n",
    "\n",
    "        if parquet_files:\n",
    "            # Read all Parquet files and concatenate them into a single DataFrame\n",
    "            dfs = [\n",
    "                pd.read_parquet(os.path.join(component_folder, file))\n",
    "                for file in parquet_files\n",
    "            ]\n",
    "            return pd.concat(dfs, ignore_index=True)\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def has_parquet_file(data_directory, entry, component_name):\n",
    "    component_folder = os.path.join(data_directory, entry, component_name)\n",
    "    # Check if the component exists\n",
    "    if not os.path.exists(component_folder) or not os.path.isdir(component_folder):\n",
    "        return False\n",
    "    parquet_files = [\n",
    "        file for file in os.listdir(component_folder) if file.endswith(\".parquet\")\n",
    "    ]\n",
    "    return bool(parquet_files)\n",
    "\n",
    "\n",
    "def extract_timestamp(folder_name):\n",
    "    # Extract the timestamp part from the folder name\n",
    "    timestamp_str = folder_name.split(\"-\")[-1]\n",
    "    # Convert the timestamp string to a datetime object\n",
    "    return datetime.strptime(timestamp_str, \"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dir = \"./data-dir\"\n",
    "read_latest_data(\n",
    "            base_path=pipeline_dir,\n",
    "            pipeline_name=\"evaluation-pipeline\",\n",
    "            component_name=\"aggregate_eval_results\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-Search (not yet developed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid-search/iterative loop\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Define the values for grid search\n",
    "chunk_sizes = [128, 256, 512]\n",
    "chunk_overlaps = [10, 25, 50]\n",
    "embed_models = [(\"huggingface\",\"all-MiniLM-L6-v2\"), (\"huggingface\"\"BAAI/bge-small-en\"), (\"huggingface\"\"BAAI/bge-large-zh-v1.5\")]\n",
    "top_k = [2, 3, 5]\n",
    "\n",
    "# Fixed parameters\n",
    "hf_dataset_name = \"wikitext@~parquet\"\n",
    "data_column_name = \"text\"\n",
    "load_sample = True\n",
    "weaviate_url = \"http://host.docker.internal:8080\"\n",
    "csv_dataset_uri = \"/data/wikitext_1000_q.csv\"\n",
    "csv_column_separator = \";\"\n",
    "question_column_name = \"question\"\n",
    "llm_name = \"OpenAI\"\n",
    "llm_kwargs = {\"openai_api_key\": ''} \n",
    "metrics = [\"context_precision\", \"context_relevancy\"]\n",
    "\n",
    "# Results dictionary to store results for each iteration\n",
    "results_dict = {}\n",
    "\n",
    "# Perform grid search\n",
    "for i, chunk_size, chunk_overlap, embed_model in enumerate(itertools.product(chunk_sizes, chunk_overlaps, embed_models, top_k), start=1):\n",
    "    # Call the run_evaluation_pipeline function with the current parameter combination\n",
    "    indexing_class_name = f\"Index_{i}\"\n",
    "    logging.info(f\"Running indexing for iteration {i} with chunk_size={chunk_size}, chunk_overlap={chunk_overlap}, embed_model={embed_model}\")\n",
    "    \n",
    "    run_indexing_pipeline(\n",
    "        hf_dataset_name=hf_dataset_name,\n",
    "        data_column_name=data_column_name,\n",
    "        load_sample=load_sample,\n",
    "        chunk_size=chunk_sizes,\n",
    "        chunk_overlap=chunk_overlaps,\n",
    "        embed_model_provider=embed_models[0],\n",
    "        embed_model=embed_models[1],\n",
    "        weaviate_url=weaviate_url,\n",
    "        weaviate_class_name=indexing_class_name\n",
    "    )\n",
    "\n",
    "    # Run evaluation pipeline\n",
    "    evaluation_class_name = f\"Index_{i}\"\n",
    "    logging.info(f\"Running evaluation for iteration {i} with top_k={top_k}\")\n",
    "\n",
    "    results = run_evaluation_pipeline(\n",
    "        csv_dataset_uri=csv_dataset_uri,\n",
    "        csv_column_separator=csv_column_separator,\n",
    "        question_column_name=question_column_name,\n",
    "        embed_model_provider=embed_models[0],\n",
    "        embed_model=embed_models[1],\n",
    "        weaviate_url=weaviate_url,\n",
    "        weaviate_class_name=indexing_class_name,\n",
    "        top_k=top_k,  # Set your desired top_k value\n",
    "        llm_name=llm_name,\n",
    "        llm_kwargs=llm_kwargs\n",
    "        metrics=metrics  # Set your desired metrics\n",
    "    )\n",
    "\n",
    "    # Save the results in the dictionary\n",
    "    results_dict[(chunk_size, chunk_overlap, embed_model, top_k)] = results #results should be a dictionary with {\"metric 1\": x, \"metric 2\": y}\n",
    "\n",
    "# Print or use the results as needed\n",
    "print(results_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
