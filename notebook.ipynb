{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🍫 Building a RAG indexing pipeline with Fondant\n",
    "\n",
    "This repository demonstrates a Fondant data pipeline that ingests text\n",
    "data into a vector database. The pipeline uses four reusable Fondant components.  \n",
    "Additionally, we provide a Docker Compose setup for Weaviate, enabling local testing and\n",
    "development.\n",
    "\n",
    "### Pipeline overview\n",
    "\n",
    "The primary goal of this sample is to showcase how you can use a Fondant pipeline and reusable\n",
    "components to load, chunk and embed text, as well as ingest the text embeddings to a vector\n",
    "database.\n",
    "Pipeline Steps:\n",
    "\n",
    "- [Data Loading](https://github.com/ml6team/fondant/tree/main/components/load_from_parquet): The\n",
    "  pipeline begins by loading text data from a Parquet file, which serves as the\n",
    "  source for subsequent processing. For the minimal example we are using a dataset from Huggingface.\n",
    "- [Text Chunking](https://github.com/ml6team/fondant/tree/main/components/chunk_text): Text data is\n",
    "  chunked into manageable sections to prepare it for embedding. This\n",
    "  step\n",
    "  is crucial for performant RAG systems.\n",
    "- [Text Embedding](https://github.com/ml6team/fondant/tree/main/components/embed_text): We are using\n",
    "  a small HuggingFace model for the generation of text embeddings.\n",
    "  The `embed_text` component easily allows the usage of different models as well.\n",
    "- [Write to Weaviate](https://github.com/ml6team/fondant/tree/main/components/index_weaviate): The\n",
    "  final step of the pipeline involves writing the embedded text data to\n",
    "  a Weaviate database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fondant[docker]==0.6.2 in ./.venv/lib/python3.10/site-packages (0.6.2)\n",
      "Requirement already satisfied: jsonschema>=4.18 in ./.venv/lib/python3.10/site-packages (from fondant[docker]==0.6.2) (4.19.1)\n",
      "Requirement already satisfied: pyarrow>=11.0.0 in ./.venv/lib/python3.10/site-packages (from fondant[docker]==0.6.2) (13.0.0)\n",
      "Requirement already satisfied: dask[dataframe,diagnostics,distributed]>=2023.4.1 in ./.venv/lib/python3.10/site-packages (from fondant[docker]==0.6.2) (2023.10.0)\n",
      "Requirement already satisfied: pandas>=1.3.5 in ./.venv/lib/python3.10/site-packages (from fondant[docker]==0.6.2) (2.1.1)\n",
      "Requirement already satisfied: docker>=6.1.3 in ./.venv/lib/python3.10/site-packages (from fondant[docker]==0.6.2) (6.1.3)\n",
      "Requirement already satisfied: click>=8.0 in ./.venv/lib/python3.10/site-packages (from dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in ./.venv/lib/python3.10/site-packages (from dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (3.0.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in ./.venv/lib/python3.10/site-packages (from dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (2023.10.0)\n",
      "Requirement already satisfied: toolz>=0.10.0 in ./.venv/lib/python3.10/site-packages (from dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (0.12.0)\n",
      "Requirement already satisfied: partd>=1.2.0 in ./.venv/lib/python3.10/site-packages (from dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (1.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in ./.venv/lib/python3.10/site-packages (from dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (6.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in ./.venv/lib/python3.10/site-packages (from dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (23.2)\n",
      "Requirement already satisfied: bokeh>=2.4.2 in ./.venv/lib/python3.10/site-packages (from dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (3.3.0)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in ./.venv/lib/python3.10/site-packages (from dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (3.1.2)\n",
      "Requirement already satisfied: distributed==2023.10.0 in ./.venv/lib/python3.10/site-packages (from dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (2023.10.0)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in ./.venv/lib/python3.10/site-packages (from distributed==2023.10.0->dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (1.0.7)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in ./.venv/lib/python3.10/site-packages (from distributed==2023.10.0->dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (2.4.0)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in ./.venv/lib/python3.10/site-packages (from distributed==2023.10.0->dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (2.0.7)\n",
      "Requirement already satisfied: zict>=3.0.0 in ./.venv/lib/python3.10/site-packages (from distributed==2023.10.0->dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (3.0.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in ./.venv/lib/python3.10/site-packages (from distributed==2023.10.0->dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.0.4 in ./.venv/lib/python3.10/site-packages (from distributed==2023.10.0->dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (6.3.3)\n",
      "Requirement already satisfied: locket>=1.0.0 in ./.venv/lib/python3.10/site-packages (from distributed==2023.10.0->dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (1.0.0)\n",
      "Requirement already satisfied: psutil>=5.7.2 in ./.venv/lib/python3.10/site-packages (from distributed==2023.10.0->dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (5.9.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.venv/lib/python3.10/site-packages (from docker>=6.1.3->fondant[docker]==0.6.2) (2.31.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in ./.venv/lib/python3.10/site-packages (from docker>=6.1.3->fondant[docker]==0.6.2) (1.6.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.10/site-packages (from jsonschema>=4.18->fondant[docker]==0.6.2) (23.1.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.10/site-packages (from jsonschema>=4.18->fondant[docker]==0.6.2) (0.10.6)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.10/site-packages (from jsonschema>=4.18->fondant[docker]==0.6.2) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.10/site-packages (from jsonschema>=4.18->fondant[docker]==0.6.2) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.venv/lib/python3.10/site-packages (from pandas>=1.3.5->fondant[docker]==0.6.2) (1.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas>=1.3.5->fondant[docker]==0.6.2) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.10/site-packages (from pandas>=1.3.5->fondant[docker]==0.6.2) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas>=1.3.5->fondant[docker]==0.6.2) (2023.3.post1)\n",
      "Requirement already satisfied: pillow>=7.1.0 in ./.venv/lib/python3.10/site-packages (from bokeh>=2.4.2->dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (10.1.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in ./.venv/lib/python3.10/site-packages (from bokeh>=2.4.2->dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (2023.10.0)\n",
      "Requirement already satisfied: contourpy>=1 in ./.venv/lib/python3.10/site-packages (from bokeh>=2.4.2->dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.venv/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2>=2.10.3->dask[dataframe,diagnostics,distributed]>=2023.4.1->fondant[docker]==0.6.2) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->fondant[docker]==0.6.2) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.26.0->docker>=6.1.3->fondant[docker]==0.6.2) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.26.0->docker>=6.1.3->fondant[docker]==0.6.2) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.26.0->docker>=6.1.3->fondant[docker]==0.6.2) (3.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Setup your environment \n",
    "!pip install \"fondant[docker]==0.6.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the pipeline\n",
    "\n",
    "First of all, we need to initialize the pipeline, which includes specifying a name for your pipeline, providing a description, and setting a base_path. The base_path is used to store the pipeline artifacts and data generated by the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.pipeline import ComponentOp, Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    pipeline_name=\"ingestion-pipeline\",  # Add a unique pipeline name to easily track your progress and data\n",
    "    pipeline_description=\"Pipeline to prepare and process data for building a RAG solution\",\n",
    "    base_path=\"./data-dir\", # The demo pipelines uses a local directory to store the data.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, we will utilize a dataset available on Hugging Face. As such, we will use a reusable Fondant component `load_from_hf_hub`. The `load_from_hf_hub`` component is a generic one, which implies that we still need to customize the component specification file. We have to modify the dataframe schema defined in the produce section of the component.\n",
    "\n",
    "To achieve this, we can create a `fondant_component.yaml` file in the directory `components/load_from_hf_hub` with the following content:\n",
    "\n",
    "```yaml\n",
    "name: Load from huggingface hub\n",
    "description: Component that loads a dataset from huggingface hub\n",
    "image: fndnt/load_from_hf_hub:0.6.2\n",
    "\n",
    "produces:\n",
    "  text:\n",
    "    fields:\n",
    "      data:\n",
    "        type: string\n",
    "\n",
    "args:\n",
    "  dataset_name:\n",
    "    description: Name of dataset on the hub\n",
    "    type: str\n",
    "  column_name_mapping:\n",
    "    description: Mapping of the consumed hub dataset to fondant column names\n",
    "    type: dict\n",
    "    default: {}\n",
    "  image_column_names:\n",
    "    description: Optional argument, a list containing the original image column names in case the \n",
    "      dataset on the hub contains them. Used to format the image from HF hub format to a byte string.\n",
    "    type: list\n",
    "    default: []\n",
    "  n_rows_to_load:\n",
    "    description: Optional argument that defines the number of rows to load. Useful for testing pipeline runs on a small scale\n",
    "    type: int\n",
    "    default: None\n",
    "  index_column:\n",
    "    description: Column to set index to in the load component, if not specified a default globally unique index will be set\n",
    "    type: str\n",
    "    default: None\n",
    "```\n",
    "\n",
    "Afterwards, we can initialize the component and add it to our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_hf_hub = ComponentOp(\n",
    "    component_dir=\"components/load_from_hf_hub\",\n",
    "    arguments={\n",
    "        # Add arguments\n",
    "        \"dataset_name\": \"wikitext@~parquet\",\n",
    "        # Define the column mapping between the huggingface dataset and the Fondant dataframe\n",
    "        \"column_name_mapping\": {\n",
    "            \"text\": \"text_data\"\n",
    "        },\n",
    "        \"n_rows_to_load\": 10\n",
    "    }\n",
    ")\n",
    "\n",
    "pipeline.add_op(load_from_hf_hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our pipeline consists of a single component that loads the dataset from HuggingFace Hub. We can proceed to add the other components. All of them are reusable components, and we can initialize them using the `ComponentOp.from_registry(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_text_op = ComponentOp.from_registry(\n",
    "    name=\"chunk_text\",\n",
    "    arguments={\n",
    "        \"chunk_size\": 512,\n",
    "        \"chunk_overlap\": 32,\n",
    "    }\n",
    ")\n",
    "\n",
    "embed_text_op = ComponentOp.from_registry(\n",
    "    name=\"embed_text\",\n",
    "    arguments={\n",
    "        \"model_provider\": \"huggingface\",\n",
    "        \"model\": \"all-MiniLM-L6-v2\",\n",
    "    }\n",
    ")\n",
    "\n",
    "index_weaviate_op = ComponentOp.from_registry(\n",
    "    name=\"index_weaviate\",\n",
    "    arguments={\n",
    "        \"weaviate_url\": \"http://host.docker.internal:8080\",\n",
    "        \"class_name\": \"index\",  # Add a unique class name to show up on the leaderboard\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the components in our pipeline. It is important to note that we will define dependencies between the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_op(chunk_text_op, dependencies=load_from_hf_hub)\n",
    "pipeline.add_op(embed_text_op, dependencies=chunk_text_op)\n",
    "pipeline.add_op(index_weaviate_op, dependencies=embed_text_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the pipeline\n",
    "\n",
    "The pipeline will load and process text data, then ingest the processed data into a vector database. Before executing the pipeline, we need to start the Weaviate database. Otherwise the pipeline execution will fail.\n",
    "\n",
    "To do this, we can utilize the Docker setup provided in the `weaviate` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Running 2/0\n",
      " \u001b[32m✔\u001b[0m Container weaviate-weaviate-1       \u001b[32mRunning\u001b[0m                             \u001b[34m0.0s \u001b[0m\n",
      " \u001b[32m✔\u001b[0m Container weaviate-contextionary-1  \u001b[32mRunning\u001b[0m                             \u001b[34m0.0s \u001b[0m\n",
      "\u001b[?25hAttaching to weaviate-contextionary-1, weaviate-weaviate-1\n",
      "^C\n",
      "Gracefully stopping... (press Ctrl+C again to force)\n",
      "Aborting on container exit...\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Stopping 0/0\n",
      " ⠋ Container weaviate-contextionary-1  Stopping                            \u001b[34m0.1s \u001b[0m\n",
      " ⠋ Container weaviate-weaviate-1       Stopping                            \u001b[34m0.1s \u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!docker-compose -f weaviate/docker-compose.yaml up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can execute our pipeline. Fondant provides various executors, and in this case, we are using the LocalRunner, which utilizes Docker under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-31 14:30:24,515 | fondant.compiler | INFO] Compiling ingestion-pipeline to docker-compose.yml\n",
      "[2023-10-31 14:30:24,517 | fondant.compiler | INFO] Base path found on local system, setting up ./data-dir as mount volume\n",
      "[2023-10-31 14:30:24,519 | fondant.pipeline | INFO] Sorting pipeline component graph topologically.\n",
      "[2023-10-31 14:30:24,526 | fondant.pipeline | INFO] All pipeline component specifications match.\n",
      "[2023-10-31 14:30:24,527 | fondant.compiler | INFO] Compiling service for load_from_huggingface_hub\n",
      "[2023-10-31 14:30:24,528 | fondant.compiler | INFO] Compiling service for chunk_text\n",
      "[2023-10-31 14:30:24,528 | fondant.compiler | INFO] Compiling service for embed_text\n",
      "[2023-10-31 14:30:24,529 | fondant.compiler | INFO] Compiling service for index_weaviate\n",
      "[2023-10-31 14:30:24,540 | fondant.compiler | INFO] Successfully compiled to docker-compose.yml\n",
      " load_from_huggingface_hub Pulling \n",
      " embed_text Pulling \n",
      " chunk_text Pulling \n",
      " index_weaviate Pulling \n",
      " index_weaviate Error \n",
      " load_from_huggingface_hub Error \n",
      "no matching manifest for linux/arm64/v8 in the manifest list entries\n"
     ]
    }
   ],
   "source": [
    "from fondant.compiler import DockerCompiler\n",
    "from fondant.runner import DockerRunner\n",
    "\n",
    "DockerCompiler().compile(pipeline)\n",
    "DockerRunner().run(\"docker-compose.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you save the pipeline into a python file you can execute the pipeline using the Fondant cli as well.\n",
    "\n",
    "> `fondant run local pipeline.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
